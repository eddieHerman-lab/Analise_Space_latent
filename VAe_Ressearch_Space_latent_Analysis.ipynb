{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eddieHerman-lab/Analise_Space_latent/blob/main/VAe_Ressearch_Space_latent_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VdLGWSJ1iMK"
      },
      "source": [
        "# Microscopic Analysis of the Latent Space: Heuristics for Interpretability, Authenticity, and Bias Detection in VAE Representations\n",
        "\n",
        "**Author:** Eduardo Augusto Pires Hermanson\n",
        "<br>\n",
        "**Abstract:** This notebook contains the complete code implementation for the research paper of the same name. It covers data loading, model setup, the calculation of custom heuristics (Uniqueness, Originality, CLS), clustering analysis (UMAP + HDBSCAN), and the generation of all figures and results presented in the case study on the CelebA dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8yd2szK3YR0"
      },
      "source": [
        "This cell downloads the pre-trained VAE model weights from the Hugging Face Hub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsjIa_l9E_9X"
      },
      "outputs": [],
      "source": [
        "# Paste and execute this cell in your Colab notebook\n",
        "!wget https://huggingface.co/hussamalafandi/VAE-CelebA/resolve/main/vae_celeba_latent_200_epochs_10_batch_64_subset_80000.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huklmUDe3nZK"
      },
      "source": [
        "Those preliminary cells mounts your Google Drive to the Colab environment, allowing you to access files stored in your Drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  SETUP CELL  AND  REPRODUCIBLE (VIA KAGGLE API)\n",
        "# ==============================================================================\n",
        "import os\n",
        "\n",
        "# Step 1: Upload your Kaggle API token\n",
        "from google.colab import files\n",
        "print(\"Por favor, faça o upload do seu arquivo 'kaggle.json' que você baixou do site do Kaggle.\")\n",
        "files.upload() #A window will open for you to select the file\n",
        "\n",
        "# Step 2: Configure the API\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"\\nAPI do Kaggle configurated successfully.\")\n",
        "\n",
        "# Step 3: Create the folder structure\n",
        "os.makedirs('data', exist_ok=True)\n",
        "print(\"Folder structure created.\")\n",
        "\n",
        "# Step 4: Download the complete dataset from Kaggle (ultra-fast)\n",
        "# Este dataset já inclui img_align_celeba, list_attr_celeba.csv, e identity_CelebA.txt\n",
        "print(\"\\nDownloading Kaggle CelebA dataset ...\")\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset -p data/\n",
        "print(\"Download concluded!\")\n",
        "\n",
        "# Step 5: Unzip the dataset into the 'data' folder\n",
        "print(\"\\nUnpacking files...\")\n",
        "!unzip -q data/celeba-dataset.zip -d data/\n",
        "print(\"Dataset successfully unzipped into 'data/' folder.\")\n",
        "\n",
        "# Passo 6: Verificação Final\n",
        "print(\"\\nContents of the 'data/' folder:\")\n",
        "!ls -l data/"
      ],
      "metadata": {
        "id": "zkfZ3BiuDLwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "9_qnowD3GOS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-nzYjop39no"
      },
      "source": [
        "This cell unzips a zip file containing the CelebA dataset images from your Google Drive to the specified directory in the Colab environment. Make sure to adjust the path to your zip file if it's different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bCarYpT1tcQ"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        " Definition of  the `EyeVAE` class, a convolutional variational autoencoder architecture. It includes the encoder to map images to a latent space, the reparameterization trick for sampling from the latent space, and the decoder to reconstruct images from the latent space. It also defines helper classes for `ComponentDecomposer`, `LatentSpaceAnalyzer`, `EntropicOriginalityMeasure`, and `TemporalStabilityAnalyzer` which implement various heuristic calculations for analyzing the latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbV7bGcRokVH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.stats import entropy, kurtosis # Import kurtosis\n",
        "from sklearn.decomposition import FastICA, FactorAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.signal import find_peaks\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import hdbscan\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EyeVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional version (CVAE) of EyeVAE, compatible with pre-trained models\n",
        "    on images like CelebA.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels=3, latent_dim=200):\n",
        "        super(EyeVAE, self).__init__()\n",
        "\n",
        "        # --- ENCODER ---\n",
        "        # Convolutions to extract spatial features from the image\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1), # -> 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),            # -> 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),           # -> 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),          # -> 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten() # Flatten the 4x4x256 feature map into a vector\n",
        "        )\n",
        "\n",
        "        # Linear layers to map extracted features to the latent space\n",
        "        # Size 4096 comes from 256 * 4 * 4\n",
        "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
        "\n",
        "        # --- DECODER ---\n",
        "        # Linear layer to prepare the latent vector for reconstruction\n",
        "        self.decoder_input = nn.Linear(latent_dim, 256 * 4 * 4)\n",
        "\n",
        "        # Transposed convolutions to \"draw\" the image back from features\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # -> 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # -> 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # -> 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, input_channels, kernel_size=4, stride=2, padding=1), # -> 64x64\n",
        "            nn.Sigmoid() # Ensure output pixels are between 0 and 1\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        result = self.encoder(x)\n",
        "        mu = self.fc_mu(result)\n",
        "        logvar = self.fc_logvar(result)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 256, 4, 4) # Reshape from flattened for convolutional layers\n",
        "        result = self.decoder(result)\n",
        "        return result\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "class ComponentDecomposer:\n",
        "    \"\"\"Decomposition of independent components for uniqueness analysis\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        self.embeddings = embeddings\n",
        "        self.ica_components = None\n",
        "        self.factor_components = None\n",
        "        self.unique_signatures = None\n",
        "\n",
        "    def decompose_independent_factors(self, n_components=None, n_init_simulation=10):\n",
        "        \"\"\"\n",
        "        Decomposition into independent factors, simulating n_init for greater robustness.\n",
        "        \"\"\"\n",
        "        print(f\"Performing independent component analysis (simulating n_init={n_init_simulation} times)...\")\n",
        "\n",
        "        if n_components is None:\n",
        "            # Limit components to avoid issues with small datasets or high dimensionality\n",
        "            n_components = min(20, self.embeddings.shape[1])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        embeddings_scaled = scaler.fit_transform(self.embeddings)\n",
        "\n",
        "        best_ica_components = None\n",
        "        best_avg_kurtosis = -1  # Kurtosis of a Gaussian is 0, so we start below that\n",
        "\n",
        "\n",
        "        for i in range(n_init_simulation):\n",
        "            # We create a new instance in each loop for a new random initialization\n",
        "            ica = FastICA(n_components=n_components,fun= 'cube', max_iter=8000, tol=1e-4,algorithm='deflation',whiten='unit-variance')\n",
        "            try:\n",
        "                ica_transformed = ica.fit_transform(embeddings_scaled)\n",
        "\n",
        "                # We use kurtosis as a metric of \"non-Gaussianity\" to evaluate separation quality\n",
        "                avg_kurtosis = np.mean(np.abs(kurtosis(ica_transformed, axis=0)))\n",
        "\n",
        "                if avg_kurtosis > best_avg_kurtosis:\n",
        "                    best_avg_kurtosis = avg_kurtosis\n",
        "                    best_ica_components = ica_transformed\n",
        "                    # print(f\"  Run {i+1}/{n_init_simulation}: New best result found with average kurtosis of {avg_kurtosis:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # Catch ConvergenceWarning or other errors without breaking the code\n",
        "                # print(f\"  Run {i+1}/{n_init_simulation}: Did not converge or encountered an error. Skipping.\")\n",
        "                continue\n",
        "\n",
        "        if best_ica_components is None:\n",
        "            print(\"WARNING: FastICA failed to converge in any of the attempts.\")\n",
        "            # As a fallback, run one last time with more iterations\n",
        "            ica['max_iter'] = 15000 # Even more iterations for fallback\n",
        "            ica = FastICA(**ica)\n",
        "            try:\n",
        "                self.ica_components = ica.fit_transform(embeddings_scaled)\n",
        "            except Exception as e:\n",
        "                 print(f\"Fallback ICA also failed: {e}\")\n",
        "                 self.ica_components = embeddings_scaled[:, :n_components] # Use PCA components as last resort\n",
        "                 print(\"Using PCA components as fallback.\")\n",
        "\n",
        "        else:\n",
        "            self.ica_components = best_ica_components\n",
        "\n",
        "        # Factor analysis can remain the same\n",
        "        fa = FactorAnalysis(n_components=n_components, random_state=42)\n",
        "        self.factor_components = fa.fit_transform(embeddings_scaled)\n",
        "\n",
        "        return self.ica_components, self.factor_components\n",
        "\n",
        "    def calculate_component_uniqueness(self):\n",
        "        \"\"\"Calculates uniqueness based on components\"\"\"\n",
        "        if self.ica_components is None:\n",
        "            print(\"ICA components not available. Run decompose_independent_factors first.\")\n",
        "            return None\n",
        "\n",
        "        uniqueness_scores = []\n",
        "\n",
        "        # Pre-calculate the full similarity matrix if ICA components are not too large\n",
        "        if self.ica_components.shape[0] * self.ica_components.shape[0] < 1e8: # Avoids excessive memory usage\n",
        "            full_similarity_matrix = cosine_similarity(self.ica_components)\n",
        "        else:\n",
        "            full_similarity_matrix = None\n",
        "            print(\"Similarity matrix too large to pre-calculate. Calculating distances on the fly.\")\n",
        "\n",
        "\n",
        "        for i in range(len(self.ica_components)):\n",
        "            component = self.ica_components[i]\n",
        "\n",
        "            # Calculate \"unique signature\" based on:\n",
        "            # 1. Entropy of components\n",
        "            # Add a small epsilon to avoid log(0)\n",
        "            component_entropy = entropy(np.abs(component) + 1e-8)\n",
        "\n",
        "            # 2. Average distance to neighbors\n",
        "            if full_similarity_matrix is not None:\n",
        "                distances = 1 - full_similarity_matrix[i] # Use pre-calculated distances\n",
        "            else:\n",
        "                distances = cdist([component], self.ica_components)[0]\n",
        "\n",
        "            # Remove distance to self (which is 0)\n",
        "            distances = distances[distances > 1e-6] # Use a small threshold instead of 0 to handle floating point inaccuracies\n",
        "\n",
        "            avg_distance = np.mean(distances) if len(distances) > 0 else 0\n",
        "\n",
        "            # 3. Local variance\n",
        "            local_variance = np.var(component)\n",
        "\n",
        "            # Combined uniqueness score\n",
        "            uniqueness = component_entropy *  (1 + avg_distance) * (1 + local_variance) # Use (1 + ...) to avoid multiplying by zero\n",
        "            uniqueness_scores.append(uniqueness)\n",
        "\n",
        "        return np.array(uniqueness_scores)\n",
        "\n",
        "    def find_authentic_cores(self, threshold_percentile=80):\n",
        "        \"\"\"Finds potential authentic cores\"\"\"\n",
        "        uniqueness = self.calculate_component_uniqueness()\n",
        "        if uniqueness is None:\n",
        "            return None\n",
        "\n",
        "        # Authentic cores = high uniqueness + stability\n",
        "        threshold = np.percentile(uniqueness, threshold_percentile)\n",
        "        authentic_candidates = np.where(uniqueness > threshold)[0]\n",
        "\n",
        "        # Temporal stability analysis (based on neighboring components)\n",
        "        stable_cores = []\n",
        "        # Re-calculate or use pre-calculated similarity if available\n",
        "        if self.ica_components.shape[0] * self.ica_components.shape[0] < 1e8: # Avoids excessive memory usage\n",
        "             full_similarity_matrix = cosine_similarity(self.ica_components)\n",
        "        else:\n",
        "             full_similarity_matrix = None\n",
        "             print(\"Similarity matrix too large to pre-calculate for core validation. Calculating on the fly.\")\n",
        "\n",
        "\n",
        "        for candidate in authentic_candidates:\n",
        "            # Calculate stability based on similar components\n",
        "            component = self.ica_components[candidate]\n",
        "\n",
        "            if full_similarity_matrix is not None:\n",
        "                 similarities = full_similarity_matrix[candidate] # Use the pre-calculated row\n",
        "            else:\n",
        "                 similarities = cosine_similarity([component], self.ica_components)[0]\n",
        "\n",
        "\n",
        "            # Stable core = few very close neighbors\n",
        "            # Count neighbors above a similarity threshold, excluding the point itself\n",
        "            close_neighbors = np.sum(similarities > 0.95) - 1 # Increased similarity threshold for 'very close'\n",
        "\n",
        "            if close_neighbors <= 7:  # Isolated core = potentially authentic (adjusted threshold)\n",
        "                stable_cores.append({\n",
        "                    'index': candidate,\n",
        "                    'uniqueness': uniqueness[candidate],\n",
        "                    'neighbors': close_neighbors,\n",
        "                    'component': component\n",
        "                })\n",
        "\n",
        "        return stable_cores\n",
        "    #2part\n",
        "\n",
        "class LatentSpaceAnalyzer:\n",
        "    \"\"\"Latent space analyzer to detect overlaps and patterns\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, threshold_similarity=0.99):\n",
        "        self.embeddings = embeddings\n",
        "        self.threshold_similarity = threshold_similarity\n",
        "        self.clusters = None\n",
        "        self.density_map = None\n",
        "\n",
        "    def calculate_density_map(self, k=10):\n",
        "        \"\"\"\n",
        "        Calculates an optimized latent space density map using k-Nearest Neighbors.\n",
        "        The density score is the inverse of the average distance to the k nearest neighbors.\n",
        "        \"\"\"\n",
        "        print(f\"Calculating optimized density map with k={k}...\")\n",
        "\n",
        "        # Ensure k is not greater than the number of samples\n",
        "        if k >= len(self.embeddings):\n",
        "            print(f\"Warning: k ({k}) is greater than or equal to the number of samples ({len(self.embeddings)}). Adjusting k.\")\n",
        "            k = len(self.embeddings) - 1\n",
        "            if k < 1:\n",
        "                 print(\"Error: Not enough samples to calculate density.\")\n",
        "                 self.density_map = np.zeros(len(self.embeddings)) # Return zeros if no valid neighbors\n",
        "                 return self.density_map\n",
        "\n",
        "\n",
        "        # Configure the NearestNeighbors model. k+1 because it includes itself as a neighbor.\n",
        "        # Increased leaf_size for potentially better performance with larger datasets\n",
        "        neighbors_model = NearestNeighbors(n_neighbors=k + 1, algorithm='kd_tree', leaf_size=30, n_jobs=-1)\n",
        "        neighbors_model.fit(self.embeddings)\n",
        "\n",
        "        # Find the k+1 neighbors for all points at once.\n",
        "        # 'distances' will be a matrix where each row contains the distances to the neighbors of that point.\n",
        "        distances, indices = neighbors_model.kneighbors(self.embeddings)\n",
        "\n",
        "        # The first column (index 0) is the distance from the point to itself (0.0), so we ignore it.\n",
        "        # We calculate the average of the distances to the k true neighbors (indices 1 to k+1).\n",
        "        # Add a small epsilon to the mean distance before taking the inverse to avoid division by zero\n",
        "        mean_distances = np.mean(distances[:, 1:], axis=1) + 1e-8\n",
        "\n",
        "        # Density is the inverse of the average distance.\n",
        "        densities = 1.0 / mean_distances\n",
        "\n",
        "        # Normalize densities to a 0-1 range for better comparison, optional\n",
        "        # densities = (densities - np.min(densities)) / (np.max(densities) - np.min(densities) + 1e-8)\n",
        "\n",
        "\n",
        "        self.density_map = np.array(densities)\n",
        "        return self.density_map\n",
        "\n",
        "\n",
        "    def find_potential_overlaps(self):\n",
        "        \"\"\"Finds potential overlaps in the latent space\"\"\"\n",
        "        print(\"Searching for potential overlaps...\")\n",
        "        # Use euclidean_distances as it's often more intuitive for 'overlap' in space\n",
        "        distances = euclidean_distances(self.embeddings)\n",
        "\n",
        "        # Find pairs with small distance (excluding diagonal)\n",
        "        np.fill_diagonal(distances, np.inf) # Fill diagonal with infinity so it's not picked up by < threshold\n",
        "        close_pairs_indices = np.where(distances < (1 - self.threshold_similarity)) # Use distance threshold based on similarity\n",
        "\n",
        "        overlaps = []\n",
        "        for i, j in zip(close_pairs_indices[0], close_pairs_indices[1]):\n",
        "            if i < j:  # avoid duplicates\n",
        "                # Recalculate similarity for clarity in output if needed, or just report distance\n",
        "                similarity = cosine_similarity([self.embeddings[i]], [self.embeddings[j]])[0][0]\n",
        "                overlaps.append({\n",
        "                    'pair': (i, j),\n",
        "                    'similarity': similarity, # Keep similarity for context\n",
        "                    'distance': distances[i, j]\n",
        "                })\n",
        "\n",
        "        # Sort by distance (smaller distance = higher overlap)\n",
        "        return sorted(overlaps, key=lambda x: x['distance'])\n",
        "\n",
        "    def cluster_analysis(self, min_cluster_size=0.5, min_samples=None, cluster_selection_epsilon=0.0, max_clusters=20):\n",
        "        \"\"\"\n",
        "        Performs robust cluster analysis using HDBSCAN to find density regions.\n",
        "        \"\"\"\n",
        "\n",
        "        # HDBSCAN is sensitive to scale, so standardizing data is good practice.\n",
        "        scaler = StandardScaler()\n",
        "        embeddings_scaled = scaler.fit_transform(self.embeddings)\n",
        "\n",
        "        # Configure the HDBSCAN model.\n",
        "        # min_cluster_size: The smallest group of points you consider a \"cluster\".\n",
        "        # min_samples: Controls how conservative the algorithm is. If None, the default is equal to min_cluster_size.\n",
        "        # cluster_selection_epsilon: Used to \"flatten\" the hierarchy and merge nearby clusters. 0.0 is a good default.\n",
        "        clusterer = hdbscan.HDBSCAN(\n",
        "            min_cluster_size=min_cluster_size,\n",
        "            min_samples=min_samples,\n",
        "            cluster_selection_epsilon=cluster_selection_epsilon,\n",
        "            metric='euclidean',\n",
        "            allow_single_cluster=True # Allow finding a single cluster if data is not well-separated\n",
        "        )\n",
        "        print(f\"Performing robust cluster analysis with HDBSCAN (min_cluster_size={min_cluster_size})...\")\n",
        "        # Execute clustering.\n",
        "        # Labels (-1 for noise, 0, 1, 2... for clusters) are assigned to each point.\n",
        "        try:\n",
        "            cluster_labels = clusterer.fit_predict(embeddings_scaled)\n",
        "            self.clusters = cluster_labels\n",
        "\n",
        "            # Useful information for analysis\n",
        "            unique_clusters = set(cluster_labels)\n",
        "            n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
        "            n_noise = list(cluster_labels).count(-1)\n",
        "\n",
        "            print(f'HDBSCAN found {n_clusters} cluster(s) and {n_noise} noise point(s).')\n",
        "\n",
        "            return self.clusters\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during HDBSCAN clustering: {e}\")\n",
        "            print(\"Falling back to KMeans clustering (less robust to noise and density variations).\")\n",
        "            # Fallback to KMeans if HDBSCAN fails\n",
        "            n_clusters_kmeans = min(max_clusters, len(self.embeddings) // min_cluster_size) # Estimate a reasonable number of clusters\n",
        "            if n_clusters_kmeans < 2:\n",
        "                 print(\"Not enough data points to form clusters.\")\n",
        "                 self.clusters = np.zeros(len(self.embeddings)) -1 # Label all as noise\n",
        "                 return self.clusters\n",
        "\n",
        "            kmeans = KMeans(n_clusters=n_clusters_kmeans, random_state=42, n_init=10) # Use n_init to improve robustness\n",
        "            self.clusters = kmeans.fit_predict(embeddings_scaled)\n",
        "            print(f'KMeans found {n_clusters_kmeans} cluster(s).')\n",
        "            return self.clusters\n",
        "\n",
        "\n",
        "    def _find_elbow(self, inertias):\n",
        "        \"\"\"Finds the elbow point for K-means\"\"\"\n",
        "        # Simple method: largest second derivative difference\n",
        "        if len(inertias) < 3:\n",
        "            return 0\n",
        "\n",
        "        # Ensure inertias are a numpy array for calculations\n",
        "        inertias = np.array(inertias)\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        # The elbow is where the rate of decrease changes most significantly, often at the peak of the second derivative\n",
        "        # Add 1 to the index because diff reduces the array size by 1\n",
        "        return np.argmax(second_diffs) + 1 # Return the index for the *number* of clusters\n",
        "\n",
        "\n",
        "    def analyze_cluster_characteristics(self, attributes_df):\n",
        "        \"\"\"Analyzes the characteristics of the found clusters, including attribute distribution.\"\"\"\n",
        "        if self.clusters is None:\n",
        "            print(\"Run cluster_analysis first!\")\n",
        "            return None\n",
        "\n",
        "        unique_clusters = np.unique(self.clusters)\n",
        "        cluster_stats = {}\n",
        "\n",
        "        for cluster_id in unique_clusters:\n",
        "            # Exclude noise points if using HDBSCAN\n",
        "            if cluster_id == -1:\n",
        "                continue\n",
        "\n",
        "            cluster_points = self.embeddings[self.clusters == cluster_id]\n",
        "\n",
        "            if len(cluster_points) == 0:\n",
        "                 continue # Skip empty clusters\n",
        "\n",
        "            centroid = np.mean(cluster_points, axis=0)\n",
        "\n",
        "            # Calculate internal dispersion\n",
        "            # Use cdist for potentially faster distance calculation\n",
        "            distances_to_centroid = cdist([centroid], cluster_points)[0]\n",
        "\n",
        "\n",
        "            cluster_stats[cluster_id] = {\n",
        "                'size': len(cluster_points),\n",
        "                'centroid': centroid,\n",
        "                'avg_internal_distance': np.mean(distances_to_centroid),\n",
        "                'max_internal_distance': np.max(distances_to_centroid),\n",
        "                'compactness': np.std(distances_to_centroid) # Standard deviation as compactness measure\n",
        "            }\n",
        "\n",
        "            # Attribute Distribution (Assuming attributes_df is aligned with embeddings)\n",
        "            cluster_attribute_means = attributes_df[self.clusters == cluster_id].mean()\n",
        "            cluster_stats[cluster_id]['attribute_means'] = cluster_attribute_means\n",
        "\n",
        "        return cluster_stats\n",
        "#3part\n",
        "class EntropicOriginalityMeasure:\n",
        "    \"\"\"Entropic originality measure - 'addressing' of attractors\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        self.embeddings = embeddings\n",
        "        self.attractor_map = None\n",
        "        self.originality_scores = None\n",
        "\n",
        "    def map_attractors(self, radius=0.01):\n",
        "        \"\"\"Maps attractors in the latent space\"\"\"\n",
        "        print(\"Mapping attractors in the latent space...\")\n",
        "\n",
        "        # Use NearestNeighbors for efficient radius search\n",
        "        # n_neighbors = len(self.embeddings) # Or a smaller number if performance is an issue\n",
        "        # neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', n_jobs=-1)\n",
        "        # neighbors_model.fit(self.embeddings)\n",
        "        # distances, indices = neighbors_model.kneighbors(self.embeddings)\n",
        "\n",
        "        # Identify attractors as points with many close neighbors\n",
        "        attractors = []\n",
        "        # Use radius_neighbors for finding points within a given radius\n",
        "        neighbors_model = NearestNeighbors(radius=radius, algorithm='auto', n_jobs=-1)\n",
        "        neighbors_model.fit(self.embeddings)\n",
        "        # Find all points within the specified radius for each point\n",
        "        radius_neighbors = neighbors_model.radius_neighbors(self.embeddings)\n",
        "\n",
        "\n",
        "        for i in range(len(self.embeddings)):\n",
        "            # Get indices of neighbors within the radius\n",
        "            neighbor_indices = radius_neighbors[0][i]\n",
        "            # Exclude the point itself\n",
        "            num_neighbors = len(neighbor_indices) - 1\n",
        "\n",
        "            density = num_neighbors / len(self.embeddings)\n",
        "\n",
        "            attractors.append({\n",
        "                'index': i,\n",
        "                'neighbors': num_neighbors,\n",
        "                'density': density,\n",
        "                'position': self.embeddings[i]\n",
        "            })\n",
        "\n",
        "        # Order by density (stronger attractors first)\n",
        "        self.attractor_map = sorted(attractors, key=lambda x: x['density'], reverse=True)\n",
        "        return self.attractor_map\n",
        "\n",
        "    def calculate_entropic_originality(self):\n",
        "        \"\"\"Calculates entropic originality - measures unique 'addressing'\"\"\"\n",
        "        if self.attractor_map is None:\n",
        "            self.map_attractors()\n",
        "\n",
        "        originality_scores = []\n",
        "\n",
        "        for point_data in self.attractor_map:\n",
        "            i = point_data['index']\n",
        "            point = self.embeddings[i]\n",
        "\n",
        "            # 1. Local entropy (diversity in the neighborhood)\n",
        "            # Use NearestNeighbors to find k nearest neighbors efficiently\n",
        "            k = min(10, len(self.embeddings) - 1) # Ensure k is valid\n",
        "            if k <= 0: # Handle case with very few embeddings\n",
        "                 spatial_entropy = 0\n",
        "                 neighbor_distances = []\n",
        "            else:\n",
        "                neighbors_model = NearestNeighbors(n_neighbors=k + 1, algorithm='auto', n_jobs=-1)\n",
        "                neighbors_model.fit(self.embeddings)\n",
        "                distances, indices = neighbors_model.kneighbors([point])\n",
        "                neighbor_distances = distances[0][1:] # Exclude distance to self\n",
        "\n",
        "                # Entropy of distances (spatial diversity)\n",
        "                # Ensure bins are appropriate for the range of distances\n",
        "                if len(neighbor_distances) > 1:\n",
        "                     hist, _ = np.histogram(neighbor_distances, bins='auto') # Use 'auto' for better bin selection\n",
        "                     spatial_entropy = entropy(hist + 1e-8)\n",
        "                else:\n",
        "                     spatial_entropy = 0 # Cannot calculate entropy with only one distance\n",
        "\n",
        "\n",
        "            # 2. Spectral entropy (diversity in components)\n",
        "            # Add a small epsilon to avoid log(0)\n",
        "            spectral_entropy = entropy(np.abs(point) + 1e-8)\n",
        "\n",
        "            # 3. \"Addressing\" - how unique is this point\n",
        "            # Based on information theory: more unique points have higher entropy\n",
        "            addressing_score = spatial_entropy * spectral_entropy\n",
        "\n",
        "            # 4. Isolation factor (distance to the nearest attractor)\n",
        "            if len(self.attractor_map) > 1:\n",
        "                # Find the distance to the nearest *other* attractor efficiently\n",
        "                other_attractor_positions = np.array([a['position'] for a in self.attractor_map if a['index'] != i])\n",
        "                if len(other_attractor_positions) > 0:\n",
        "                    distances_to_other_attractors = cdist([point], other_attractor_positions)[0]\n",
        "                    min_attractor_dist = np.min(distances_to_other_attractors)\n",
        "                else:\n",
        "                    min_attractor_dist = 1.0 # Default if no other attractors\n",
        "\n",
        "                isolation_factor = min_attractor_dist\n",
        "            else:\n",
        "                isolation_factor = 1.0 # Default if only one attractor\n",
        "\n",
        "            # Final originality score\n",
        "            originality = addressing_score * (1 + isolation_factor) # Use (1 + ...) for better scaling\n",
        "            originality_scores.append(originality)\n",
        "\n",
        "        # Ensure the order of scores matches the original embeddings, not the sorted attractor_map\n",
        "        # Create a mapping from attractor_map index to original index\n",
        "        original_indices = [a['index'] for a in self.attractor_map]\n",
        "        # Create an array for scores ordered by original index\n",
        "        ordered_originality_scores = np.zeros(len(self.embeddings))\n",
        "        for i, original_idx in enumerate(original_indices):\n",
        "             ordered_originality_scores[original_idx] = originality_scores[i]\n",
        "\n",
        "\n",
        "        self.originality_scores = ordered_originality_scores\n",
        "        return self.originality_scores\n",
        "\n",
        "    def identify_authentic_signatures(self, top_k=10):\n",
        "        \"\"\"Identifies potential authentic signatures\"\"\"\n",
        "        if self.originality_scores is None:\n",
        "            self.calculate_entropic_originality()\n",
        "\n",
        "        # Authentic candidates = high originality + specific density range\n",
        "        combined_scores = []\n",
        "        # Ensure we have a density map available, calculate if necessary\n",
        "        if self.attractor_map is None:\n",
        "             self.map_attractors()\n",
        "\n",
        "        density_map_dict = {a['index']: a['density'] for a in self.attractor_map}\n",
        "\n",
        "\n",
        "        for i, score in enumerate(self.originality_scores):\n",
        "            # Get density for the original index\n",
        "            density = density_map_dict.get(i, 0) # Get density using original index, default to 0 if somehow not found\n",
        "\n",
        "            # Balance originality vs. isolation\n",
        "            # Authentics should have high originality but not be too isolated\n",
        "            # Adjusted density range\n",
        "            if density > 0.005 and density < 0.15:  # Adjusted sweet spot\n",
        "                combined_scores.append((i, score, density))\n",
        "\n",
        "        # Order by originality score\n",
        "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top_k or fewer if not enough candidates\n",
        "        return combined_scores[:min(top_k, len(combined_scores))]\n",
        "\n",
        "\n",
        "class TemporalStabilityAnalyzer:\n",
        "    \"\"\"Temporal stability analysis to validate authenticity\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        self.embeddings = embeddings\n",
        "        self.stability_scores = None\n",
        "\n",
        "    def simulate_temporal_variations(self, num_variations=10, noise_level=0.005): # Reduced noise level\n",
        "        \"\"\"Simulates temporal variations to test stability\"\"\"\n",
        "        print(\"Simulating temporal variations...\")\n",
        "\n",
        "        stability_scores = []\n",
        "\n",
        "        for i, base_embedding in enumerate(self.embeddings):\n",
        "            # Generate temporal variations (simulating video frames)\n",
        "            variations = []\n",
        "            for _ in range(num_variations):\n",
        "                # Use a more realistic noise distribution or add structured noise if applicable\n",
        "                noise = np.random.normal(0, noise_level, base_embedding.shape)\n",
        "                variation = base_embedding + noise\n",
        "                variations.append(variation)\n",
        "\n",
        "            variations = np.array(variations)\n",
        "\n",
        "            # Calculate stability as consistency of variations\n",
        "            # 1. Variance of variations\n",
        "            # Use mean of variance across dimensions\n",
        "            variation_variance = np.mean(np.var(variations, axis=0))\n",
        "\n",
        "            # 2. Directional coherence\n",
        "            mean_variation = np.mean(variations, axis=0)\n",
        "            # Ensure embeddings are not zero vectors before calculating cosine similarity\n",
        "            if np.linalg.norm(base_embedding) > 1e-6 and np.linalg.norm(mean_variation) > 1e-6:\n",
        "                 directional_consistency = cosine_similarity([base_embedding], [mean_variation])[0][0]\n",
        "            else:\n",
        "                 directional_consistency = 0 # Or some other appropriate value\n",
        "\n",
        "\n",
        "            # 3. Stability score\n",
        "            # Add a small value to the denominator to avoid division by zero if variance is zero\n",
        "            stability = directional_consistency / (1 + variation_variance + 1e-8)\n",
        "            stability_scores.append(stability)\n",
        "\n",
        "        self.stability_scores = np.array(stability_scores)\n",
        "        return self.stability_scores\n",
        "\n",
        "    def validate_authentic_cores(self, candidate_cores):\n",
        "        \"\"\"Validates authentic cores through temporal stability\"\"\"\n",
        "        if self.stability_scores is None:\n",
        "            self.simulate_temporal_variations()\n",
        "\n",
        "        validated_cores = []\n",
        "\n",
        "        # Ensure stability scores are available for all original indices\n",
        "        if len(self.stability_scores) != len(self.embeddings):\n",
        "             print(\"Stability scores size mismatch with embeddings. Recalculating stability scores.\")\n",
        "             self.simulate_temporal_variations() # Recalculate if mismatch\n",
        "\n",
        "\n",
        "        for core in candidate_cores:\n",
        "            core_index = core['index']\n",
        "            # Ensure the index is valid for the stability scores array\n",
        "            if core_index < 0 or core_index >= len(self.stability_scores):\n",
        "                 print(f\"Warning: Core index {core_index} is out of bounds for stability scores.\")\n",
        "                 continue # Skip this core if index is invalid\n",
        "\n",
        "\n",
        "            stability = self.stability_scores[core_index]\n",
        "\n",
        "            # Authentic cores should have high temporal stability\n",
        "            # Adjusted percentile threshold\n",
        "            stability_threshold = np.percentile(self.stability_scores, 50) # Use 50th percentile as a more central threshold\n",
        "            if stability > stability_threshold:\n",
        "                core['temporal_stability'] = stability\n",
        "                # Combine uniqueness and stability for a validation score\n",
        "                core['validation_score'] = core['uniqueness'] * stability\n",
        "                validated_cores.append(core)\n",
        "\n",
        "        # Order by validation score\n",
        "        validated_cores.sort(key=lambda x: x['validation_score'], reverse=True)\n",
        "\n",
        "        return validated_cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qERObkMnCyfC"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# ESTRUCTURE SETUP PROJECT FOLDERS\n",
        "# This cell simulates the final structure of the repository on Github\n",
        "# ==============================================================================\n",
        "import os\n",
        "\n",
        "# Criar as pastas principais\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('pretrained_models', exist_ok=True)\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "\n",
        "print(\"Folder Structures ('data/', 'pretrained_models/', 'figures/') was sucessfully created !\")\n",
        "print(\"PLease upload the necessary files for the correct folders using te left panel.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XufdWpQi42NN"
      },
      "source": [
        " Download of the pre-trained VAE model weights from Hugging Face and loads them into the defined `EyeVAE` architecture. It then saves the loaded model weights to your Google Drive for persistent storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TwxlDAUURqR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "# Remember that the definition of the EyeVAE class must be in a previous cell\n",
        "# from model import EyeVAE\n",
        "\n",
        "# --- Downloading the pre-trained model file ---\n",
        "print(\"Downloading the pre-trained model from Hugging Face Hub...\")\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"hussamalafandi/VAE-CelebA\",\n",
        "    filename=\"vae_celeba_latent_200_epochs_10_batch_64_subset_80000.pth\"\n",
        ")\n",
        "print(f\"Download complete. Model saved to: {model_path}\")\n",
        "\n",
        "# --- Creating our architecture and loading weights ---\n",
        "print(\"\\nLoading pre-trained weights into our model...\")\n",
        "\n",
        "# We create an instance of our VAE with the correct architecture for the pre-trained model\n",
        "# The pre-trained model was trained on color images (3 channels)\n",
        "# and has a latent dimension of 200.\n",
        "latent_dim_pretrained = 200\n",
        "input_channels_celeba = 3\n",
        "# Creating an instance of the new CVAE model. It is simpler and more direct.\n",
        "our_architecture = EyeVAE(input_channels = input_channels_celeba, latent_dim = latent_dim_pretrained)\n",
        "\n",
        "# We load the weights from the downloaded file into our model instance\n",
        "# map_location='cpu' ensures it works even without an active GPU, but it will use the GPU if available\n",
        "pretrained_weights = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "our_architecture.load_state_dict(pretrained_weights)\n",
        "\n",
        "print(\"\\nPre-trained model loaded successfully!\")\n",
        "\n",
        "# The variable `our_architecture` now contains the VAE ready to be used.\n",
        "\n",
        "# Assuming 'our_architecture' is the variable containing your loaded VAE model\n",
        "\n",
        "print(\"\\nSaving the pre-trained model weights...\")\n",
        "\n",
        "# --- STEP 1: SAVE LOCALLY FIRST ---\n",
        "# Defines a path in the local and temporary Colab environment.\n",
        "local_save_path = \"/content/vae_celeba_pretrained_weights.pth\"\n",
        "\n",
        "# Saves the model's \"state dictionary\" (the weights) to this local path.\n",
        "torch.save(our_architecture.state_dict(), local_save_path)\n",
        "print(f\"Model saved successfully locally to: {local_save_path}\")\n",
        "\n",
        "\n",
        "# --- STEP 2: MOVE THE FILE TO GOOGLE DRIVE ---\n",
        "# Defines the final path in your Google Drive\n",
        "final_drive_path = \"/content/pretrained_models/vae_celeba_pretrained_weights.pth\"\n",
        "\n",
        "# Uses the terminal command 'mv' (move) to transfer the file.\n",
        "# The '!' at the beginning tells Colab this is a terminal command.\n",
        "print(f\"Moving the file to: {final_drive_path}\")\n",
        "!mv \"{local_save_path}\" \"{final_drive_path}\"\n",
        "\n",
        "print(\"\\nModel transferred to Google Drive successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLKu9v2J5au-"
      },
      "source": [
        "Defining the data transformations to preprocess the CelebA images (resizing, converting to tensor, and normalizing). It then loads the CelebA dataset using `ImageFolder` and creates a subset of 10,000 images for faster processing. Finally, it creates a `DataLoader` to efficiently feed batches of images to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XaA_jSiozEH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# --- STEP 1: Define the Transformations ---\n",
        "# Defines the sequence of preprocessing that each image will undergo.\n",
        "data_transforms = transforms.Compose([\n",
        "    # 1. Resizes all images to 64x64 pixels.\n",
        "    transforms.Resize((64, 64)),\n",
        "    # 2. Converts the PIL image to a PyTorch tensor.\n",
        "    transforms.ToTensor(),\n",
        "    # 3. Normalizes pixel values to the range [-1, 1].\n",
        "    #    (This is a common practice for VAEs pre-trained on images).\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# --- STEP 2: Load the Dataset ---\n",
        "# PyTorch's ImageFolder is smart: it finds all images in the folder we specify.\n",
        "# The path '/content/celeba_images/img_align_celeba/' should be correct.\n",
        "full_dataset = datasets.ImageFolder(root='/content/data', transform=data_transforms)\n",
        "print(f\"Full dataset found with {len(full_dataset)} images.\")\n",
        "\n",
        "# --- STEP 3: Create a Subset (for faster processing) ---\n",
        "# Working with 200,000 images is slow. Let's take a subset of 10,000 for our analysis.\n",
        "subset_indices = list(range(10000))\n",
        "subset_dataset = Subset(full_dataset, subset_indices)\n",
        "print(f\"Subset created with {len(subset_dataset)} images.\")\n",
        "\n",
        "# --- STEP 4: Create the DataLoader ---\n",
        "# The DataLoader is what will feed the model with images in batches.\n",
        "# A batch_size of 64 is a good starting point.\n",
        "data_loader = DataLoader(subset_dataset, batch_size=64, shuffle=False) # shuffle=False to maintain order\n",
        "\n",
        "print(\"\\nDataLoader ready to be used!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y72LZUfb5rOy"
      },
      "source": [
        "This cell uses the loaded VAE model to generate latent space embeddings for the subset of CelebA images. It iterates through the data loader, passes the images through the encoder to obtain the mean (`mu`) and log-variance (`logvar`) of the latent distribution, and stores the `mu` vectors as the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUCwX1kOrC1y"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Make sure your model and dataloader are defined in previous cells\n",
        "# our_architecture = ...\n",
        "# data_loader = ...\n",
        "\n",
        "# --- Preparation ---\n",
        "# Define the device (GPU, if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "our_architecture.to(device)\n",
        "our_architecture.eval() # Set the model to evaluation mode\n",
        "\n",
        "# List to store all generated embeddings\n",
        "real_embeddings = []\n",
        "\n",
        "print(f\"Starting the generation of embeddings for {len(subset_dataset)} images...\")\n",
        "\n",
        "# --- The Generation Loop ---\n",
        "# The torch.no_grad() block disables gradient calculation, making everything faster\n",
        "with torch.no_grad():\n",
        "    # tqdm gives us a progress bar to track the process\n",
        "    for images, _ in tqdm(data_loader, desc=\"Processing Batches\"):\n",
        "        # Move the batch of images to the GPU\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Pass the images through the encoder to get mu and logvar\n",
        "        mu, logvar = our_architecture.encode(images)\n",
        "\n",
        "        # The 'mu' vector is the central representation of the point in the latent space.\n",
        "        # We move it back to the CPU and add it to our list.\n",
        "        real_embeddings.append(mu.cpu())\n",
        "\n",
        "# --- Final Consolidation ---\n",
        "# Concatenate all batches of embeddings into a single giant tensor\n",
        "real_embeddings_tensor = torch.cat(real_embeddings, dim=0)\n",
        "\n",
        "# Convert the final tensor to a NumPy array, our standard format for analysis\n",
        "real_embeddings_numpy = real_embeddings_tensor.numpy()\n",
        "\n",
        "print(\"\\nProcess complete!\")\n",
        "print(f\"Shape of our real data latent space: {real_embeddings_numpy.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I32jHNez6Qjq"
      },
      "source": [
        " Performs  an initial heuristic analysis on the generated latent space embeddings. It uses the `ComponentDecomposer` (with ICA as a robust method) to get principal components, the `LatentSpaceAnalyzer` to calculate the density map and perform HDBSCAN clustering on the components. Finally, it visualizes the clusters in the 2D principal component space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvEo2cilrWXf"
      },
      "outputs": [],
      "source": [
        "# The variable `real_embeddings_numpy` already exists from the previous cell, containing the 10,000 embeddings.\n",
        "\n",
        "print(f\"\\nStarting heuristic analysis on the real data latent space ({real_embeddings_numpy.shape})...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- STEP 1: COMPONENT DECOMPOSITION (Using ICA) ---\n",
        "# We use the class we already defined, feeding it the new data.\n",
        "# Remember, we decided to use PCA for being more robust.\n",
        "decomposer = ComponentDecomposer(real_embeddings_numpy)\n",
        "principal_components, _ = decomposer.decompose_independent_factors()\n",
        "print(\"Principal component decomposition complete.\")\n",
        "\n",
        "# --- STEP 2: TOPOLOGY ANALYSIS (Density and Clusters) ---\n",
        "# We feed the analyzer with the decomposed components.\n",
        "analyzer = LatentSpaceAnalyzer(principal_components)\n",
        "density_map = analyzer.calculate_density_map()\n",
        "clusters = analyzer.cluster_analysis(min_cluster_size=15) # We can adjust min_cluster_size for real data.\n",
        "print(\"Density and cluster analysis complete.\")\n",
        "\n",
        "# --- STEP 3: AUTHENTICITY ANALYSIS AND VISUALIZATION ---\n",
        "# Here, we can run the rest of the pipeline and visualize the results.\n",
        "# (This part is more for exploration, as we don't have 'ground truth' here)\n",
        "\n",
        "print(\"\\nInitial analysis with real data complete!\")\n",
        "print(\"Now we have the components, density map, and clusters to investigate.\")\n",
        "\n",
        "# Example of how to visualize the new results (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(principal_components[:, 0], principal_components[:, 1], c=clusters, cmap='viridis', s=1)\n",
        "plt.title('Clusters in the Component Space of Real Data (CelebA)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3TixBpf6XXQ"
      },
      "source": [
        " \"Zoom-in\" analysis on the cluster with label 0 (the yellow cluster in the previous plot). It finds the indices of the images belonging to this cluster within the subset and then loads and displays a grid of these images to visually inspect their characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS4c3zyE5d8p"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the variables 'clusters' and 'subset_indices' from previous cells exist.\n",
        "# 'clusters' is the result array from HDBSCAN.\n",
        "# 'subset_dataset' is our dataset of 10,000 images.\n",
        "\n",
        "print(\"Starting the 'zoom-in' analysis of the found cluster...\")\n",
        "\n",
        "# --- Step 1: Find the indices of our cluster ---\n",
        "# We find the indices WITHIN THE SUBSET of 10,000 that belong to cluster 0 (the yellow point).\n",
        "indices_in_subset = np.where(clusters == 0)[0]\n",
        "\n",
        "if len(indices_in_subset) > 0:\n",
        "    print(f\"Found {len(indices_in_subset)} point(s) in cluster 0.\")\n",
        "\n",
        "    # --- Step 2: Load and display the corresponding images ---\n",
        "    # We need the base path for the images\n",
        "    base_image_path = \"/content/data/\"\n",
        "\n",
        "    # Get the image filenames from our subset\n",
        "    filenames = [full_dataset.samples[i][0] for i in subset_indices]\n",
        "\n",
        "    # Create a grid of subplots to display the images\n",
        "    num_images = len(indices_in_subset)\n",
        "    # Adjust the grid size for visualization\n",
        "    cols = 5\n",
        "    rows = int(np.ceil(num_images / cols))\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
        "    axes = axes.flatten() # Flatten the 2D grid into a 1D array for easier iteration\n",
        "\n",
        "    for i, idx in enumerate(indices_in_subset):\n",
        "        # Get the full image path\n",
        "        image_path = filenames[idx]\n",
        "\n",
        "        # Open the image\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Plot the image in the corresponding subplot\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"Index: {idx}\")\n",
        "        axes[i].axis('off') # Remove x and y axes\n",
        "\n",
        "    # Hide the subplots that were not used\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.suptitle(\"Images Belonging to the 'Yellow' Cluster (Label 0)\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No cluster with label 0 was found to visualize.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkgeFexo7zGo"
      },
      "source": [
        " Deep comparative analysis by initializing instances of all the defined heuristic analyzers (`ComponentDecomposer`, `LatentSpaceAnalyzer`, `EntropicOriginalityMeasure`, `TemporalStabilityAnalyzer`). It then performs topology analyses (density and clustering) and prepares to compare groups of images based on their heuristic scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFDTZcnRB0cB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# --- Requirements ---\n",
        "# Make sure these variables from your main analysis already exist:\n",
        "# - clusters: The HDBSCAN result array.\n",
        "# - subset_dataset: Our dataset of 10,000 images.\n",
        "# - full_dataset: The full CelebA dataset.\n",
        "# - principal_components: The PCA components.\n",
        "# - authenticity_decomposer: The instance of your decomposition class.\n",
        "# - entropy_analyzer: The instance of your entropy class.\n",
        "# - temporal_analyzer: The instance of your stability class.\n",
        "\n",
        "print(\"--- STARTING DEEP COMPARATIVE ANALYSIS ---\")\n",
        "\n",
        "\n",
        "print(f\"\\nStarting heuristic analysis on the real data latent space ({real_embeddings_numpy.shape})...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- STEP 1: Create ALL instances of our analyzers ---\n",
        "\n",
        "print(\"Initializing analysis modules...\")\n",
        "# The decomposer uses the raw embeddings\n",
        "decomposer = ComponentDecomposer(real_embeddings_numpy)\n",
        "principal_components, _ = decomposer.decompose_independent_factors()\n",
        "\n",
        "# The topology analyzer uses the decomposed components\n",
        "analyzer = LatentSpaceAnalyzer(principal_components)\n",
        "\n",
        "# The other heuristics also operate on the components for consistency\n",
        "entropy_analyzer = EntropicOriginalityMeasure(principal_components)\n",
        "temporal_analyzer = TemporalStabilityAnalyzer(principal_components)\n",
        "\n",
        "print(\"Modules initialized.\")\n",
        "\n",
        "# --- STEP 2: Run the main analyses ---\n",
        "\n",
        "print(\"\\nExecuting topology analyses...\")\n",
        "density_map = analyzer.calculate_density_map()\n",
        "clusters = analyzer.cluster_analysis(min_cluster_size=30) # Using the value we were testing\n",
        "\n",
        "print(\"\\nInitial analysis with real data complete!\")\n",
        "\n",
        "\n",
        "# --- Step 1: Select the Groups ---\n",
        "yellow_cluster_indices = np.where(clusters == 0)[0]\n",
        "purple_cloud_indices = np.where(clusters == -1)[0]\n",
        "\n",
        "# As you brilliantly suggested, let's take a random sample from the purple cloud\n",
        "# to be our control group.\n",
        "np.random.shuffle(purple_cloud_indices)\n",
        "purple_cloud_control_indices = purple_cloud_indices[:len(yellow_cluster_indices)] # Take the same number of samples\n",
        "\n",
        "print(f\"Group A (Yellow Cluster): {len(yellow_cluster_indices)} samples\")\n",
        "print(f\"Group B (Purple Cloud Control): {len(purple_cloud_control_indices)} samples\")\n",
        "\n",
        "# --- Step 2: Comparative Visual Analysis ---\n",
        "def plot_image_group(group_indices, graph_title):\n",
        "    if len(group_indices) == 0: return\n",
        "\n",
        "    filenames = [full_dataset.samples[i][0] for i in subset_indices]\n",
        "\n",
        "    num_images = len(group_indices)\n",
        "    cols = 5\n",
        "    rows = int(np.ceil(num_images / cols))\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, idx in enumerate(group_indices):\n",
        "        image_path = filenames[idx]\n",
        "        img = Image.open(image_path)\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"Index: {idx}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    for j in range(i + 1, len(axes)): axes[j].axis('off')\n",
        "    plt.suptitle(graph_title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n[Visualization] Group A - Images from the Yellow Cluster:\")\n",
        "plot_image_group(yellow_cluster_indices, \"Images from the Yellow Cluster (Group A)\")\n",
        "\n",
        "print(\"\\n[Visualization] Group B - Random Samples from the Purple Cloud:\")\n",
        "plot_image_group(purple_cloud_control_indices, \"Images from the Control Group (Purple Cloud - Group B)\")\n",
        "\n",
        "\n",
        "# --- Step 3: Comparative Heuristic Analysis (The Microscopic Analysis) ---\n",
        "print(\"\\n--- NUMERICAL COMPARISON OF HEURISTICS ---\")\n",
        "\n",
        "# We need the scores of all samples, which have already been calculated\n",
        "uniqueness_scores = decomposer.calculate_component_uniqueness()\n",
        "originality_scores = entropy_analyzer.calculate_entropic_originality()\n",
        "stability_scores = temporal_analyzer.simulate_temporal_variations()\n",
        "\n",
        "# Calculate means and standard deviations for each group\n",
        "group_a_scores = {\n",
        "    \"Uniqueness\": uniqueness_scores[yellow_cluster_indices],\n",
        "    \"Originality\": originality_scores[yellow_cluster_indices],\n",
        "    \"Stability\": stability_scores[yellow_cluster_indices]\n",
        "}\n",
        "group_b_scores = {\n",
        "    \"Uniqueness\": uniqueness_scores[purple_cloud_control_indices],\n",
        "    \"Originality\": originality_scores[purple_cloud_control_indices],\n",
        "    \"Stability\": stability_scores[purple_cloud_control_indices]\n",
        "}\n",
        "\n",
        "print(\"\\nAVERAGE SCORES (Mean ± Standard Deviation):\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"| Heuristic       | Group A (Yellow)         | Group B (Purple)         |\")\n",
        "print(\"-\" * 40)\n",
        "for key in group_a_scores:\n",
        "    mean_a = np.mean(group_a_scores[key])\n",
        "    std_a = np.std(group_a_scores[key])\n",
        "    mean_b = np.mean(group_b_scores[key])\n",
        "    std_b = np.std(group_b_scores[key])\n",
        "    print(f\"| {key:<15} | {mean_a:8.2f} ± {std_a:7.2f} | {mean_b:8.2f} ± {std_b:7.2f} |\")\n",
        "print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXQA8U2l8fLx"
      },
      "source": [
        "Introducing the `identity_CelebA.txt` file, which maps image filenames to identity IDs. It then counts how many images each identity has and identifies identities with at least 50 images, printing the top 5 identities with the most images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIMDAVaUUiTY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1) Load the filename → identity_id mapping\n",
        "# Each line in identity_file has something like: img_000001.jpg 100\n",
        "identity_file = \"/content/data/identity_CelebA.txt\"\n",
        "df_id = pd.read_csv(identity_file, sep=\" \", names=[\"image_id\",\"identity\"])\n",
        "\n",
        "# 2) Count how many images each identity has\n",
        "counts = df_id[\"identity\"].value_counts()\n",
        "\n",
        "# 3) Filter those with >= 50 images\n",
        "large_identities = counts[counts >= 50].index.tolist()\n",
        "print(f\"{len(large_identities)} identities with ≥50 images\")\n",
        "\n",
        "# 4) See the top-5 (largest number of images)\n",
        "top5 = counts.head(5)\n",
        "print(top5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rpDO-aP8m43"
      },
      "source": [
        "Initiating the Identity Consistency Experiment. It loads the `list_attr_celeba.csv` file containing image attributes and the `identity_CelebA.txt` file to map images to person IDs. It then selects example celebrity IDs and filters the image filenames belonging to these identities for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-0cgg-4s8Oa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"--- Starting Identity Consistency Experiment ---\")\n",
        "\n",
        "# --- Step 1: Load the attributes file ---\n",
        "# Kaggle usually puts annotation files in the main dataset folder.\n",
        "attributes_path = \"data/list_attr_celeba.csv\" # Adjust this path if necessary\n",
        "\n",
        "try:\n",
        "    df_attributes = pd.read_csv(attributes_path)\n",
        "    print(\"Attributes file loaded successfully.\")\n",
        "    # Let's see the first rows and column names\n",
        "    # print(df_attributes.head())\n",
        "    # print(df_attributes.columns)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The attributes file was not found at '{attributes_path}'.\")\n",
        "    print(\"Please check the path and ensure the .csv file is in your Drive.\")\n",
        "\n",
        "# --- Step 2: Find the IDs of our celebrities ---\n",
        "# CelebA doesn't use names, but a numerical ID for each person. We need to find out which one.\n",
        "# (This part might require manual search or using an identity mapping file if available)\n",
        "# For this example, let's assume we found the IDs (these are just examples):\n",
        "# NOTE: We will need to find the correct IDs. One way is to search for the file `identity_CelebA.txt`\n",
        "celebrity_A_id = 3745 # Example ID for Jennifer Aniston\n",
        "celebrity_B_id = 3699 # Example ID for George Clooney\n",
        "\n",
        "# --- Step 3: Filter images by identity ---\n",
        "# (This step depends on the `identity_CelebA.txt` file, which maps image_id to person_id)\n",
        "identity_path = 'data/identity_CelebA.txt'\n",
        "\n",
        "try:\n",
        "    df_identity = pd.read_csv(identity_path, sep=' ', header=None, names=['image_id', 'person_id'])\n",
        "    print(\"Identity file loaded successfully.\")\n",
        "\n",
        "    # Find the filenames for each celebrity\n",
        "    celebrity_A_images = df_identity[df_identity['person_id'] == celebrity_A_id]['image_id'].tolist()\n",
        "    celebrity_B_images = df_identity[df_identity['person_id'] == celebrity_B_id]['image_id'].tolist()\n",
        "\n",
        "    # Let's take just the first 15 from each for our test\n",
        "    celebrity_A_images = celebrity_A_images[:30]\n",
        "    celebrity_B_images = celebrity_B_images[:30]\n",
        "\n",
        "    print(f\"Found {len(celebrity_A_images)} images for Celebrity A.\")\n",
        "    print(f\"Found {len(celebrity_B_images)} images for Celebrity B.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The file 'identity_CelebA.txt' was not found at '{identity_path}'.\")\n",
        "    print(\"This file is essential for mapping images to identities. Please upload it to your Drive.\")\n",
        "\n",
        "# Now, the lists `celebrity_A_images` and `celebrity_B_images` contain the filenames\n",
        "# that we will use for our analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwhMPPjG96vb"
      },
      "source": [
        "Starting  Identity Consistency Analysis Phase. It defines a function to load and preprocess images for the VAE. It then generates latent space embeddings for the selected celebrity image groups (Celebrity A, Celebrity B, and a mixed group). Finally, it visualizes these embeddings in a 2D PCA space to show how well identities are separated in the latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAE1Y5oLRS19"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Requirements ---\n",
        "# Ensure these variables exist from previous cells:\n",
        "# - our_architecture: The pre-trained and loaded VAE model.\n",
        "# - celebrity_A_images: The list of filenames for the first celebrity.\n",
        "# - celebrity_B_images: The list of filenames for the second celebrity.\n",
        "\n",
        "print(\"--- Starting the Identity Consistency Analysis Phase ---\")\n",
        "\n",
        "# --- Step 1: Prepare a function to load and process images ---\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "base_image_path = \"/content/data/img_align_celeba/img_align_celeba\"\n",
        "\n",
        "def load_and_process_images(file_list):\n",
        "    processed_images = []\n",
        "    for filename in file_list:\n",
        "        full_path = os.path.join(base_image_path, filename)\n",
        "        try:\n",
        "            img = Image.open(full_path).convert('RGB')\n",
        "            processed_images.append(data_transforms(img))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image not found at {full_path}\")\n",
        "            continue\n",
        "    return torch.stack(processed_images)\n",
        "\n",
        "# --- Step 2: Generate Embeddings for our groups ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "our_architecture.to(device)\n",
        "our_architecture.eval()\n",
        "\n",
        "print(\"\\nGenerating embeddings for test groups...\")\n",
        "with torch.no_grad():\n",
        "    # Group A: Celebrity A only\n",
        "    tensor_A = load_and_process_images(celebrity_A_images).to(device)\n",
        "    mu_A, _ = our_architecture.encode(tensor_A)\n",
        "\n",
        "    # Group B: Celebrity B only\n",
        "    tensor_B = load_and_process_images(celebrity_B_images).to(device)\n",
        "    mu_B, _ = our_architecture.encode(tensor_B)\n",
        "\n",
        "    # Group C: Mix of the two\n",
        "    mixed_embeddings = torch.cat([mu_A, mu_B], dim=0)\n",
        "\n",
        "print(\"Embeddings generated successfully!\")\n",
        "\n",
        "# --- Step 3: Visualize the Latent Space (The Visual Proof) ---\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We use PCA to reduce dimensionality to 2D for plotting\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(mixed_embeddings.cpu().numpy())\n",
        "\n",
        "# Separate the points for coloring\n",
        "points_A = embeddings_2d[:len(mu_A)]\n",
        "points_B = embeddings_2d[len(mu_A):]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(points_A[:, 0], points_A[:, 1], c='blue', label='Celebrity A', alpha=0.7)\n",
        "plt.scatter(points_B[:, 0], points_B[:, 1], c='red', label='Celebrity B', alpha=0.7)\n",
        "plt.title('Latent Space Visualization (PCA) of Identities')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Step 4: Numerical Analysis of Consistency ---\n",
        "# (This part would be more complex, involving applying the heuristics,\n",
        "# but the visual proof will already be extremely powerful)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj1awQot-EII"
      },
      "source": [
        "Numerical analysis of identity consistency using the calculated heuristic scores. It analyzes three groups: Celebrity A (intra-identity), Celebrity B (intra-identity), and a mixed group (inter-identity). It calculates and prints the mean and standard deviation of the Uniqueness, Originality, and Stability scores for each group and performs Levene's test to compare the variances of Originality scores between groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwXnLTuZZ6hL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Requirements ---\n",
        "# Ensure these variables exist from previous cells:\n",
        "# - mu_A: The 15 embeddings of Celebrity A\n",
        "# - mu_B: The 15 embeddings of Celebrity B\n",
        "# - our_architecture: The loaded VAE model\n",
        "\n",
        "print(\"--- Starting Numerical Identity Consistency Analysis ---\")\n",
        "\n",
        "# --- Step 1: Prepare the Data ---\n",
        "# Convert to NumPy for analysis\n",
        "group_A_embeddings = mu_A.cpu().numpy()\n",
        "group_B_embeddings = mu_B.cpu().numpy()\n",
        "\n",
        "# Create Group C (Control), mixing half of each\n",
        "half = len(group_A_embeddings) // 2\n",
        "group_C_embeddings = np.vstack([group_A_embeddings[:half], group_B_embeddings[:half]])\n",
        "\n",
        "# --- Step 2: Apply the Heuristics to Each Group ---\n",
        "def analyze_group(embeddings):\n",
        "    # --- CHANGE ---\n",
        "    # We will keep decomposition only for the Uniqueness metric\n",
        "    decomposer = ComponentDecomposer(embeddings)\n",
        "    components, _ = decomposer.decompose_independent_factors()\n",
        "    uniqueness = decomposer.calculate_component_uniqueness()\n",
        "\n",
        "    # And we will apply the other heuristics on the ORIGINAL EMBEDDINGS\n",
        "    entropy_analyzer = EntropicOriginalityMeasure(embeddings) # <--- CHANGE\n",
        "    temporal_analyzer = TemporalStabilityAnalyzer(embeddings) # <--- CHANGE\n",
        "\n",
        "    originality = entropy_analyzer.calculate_entropic_originality()\n",
        "    stability = temporal_analyzer.simulate_temporal_variations()\n",
        "\n",
        "    return {\"Uniqueness\": uniqueness, \"Originality\": originality, \"Stability\": stability}\n",
        "\n",
        "print(\"\\nAnalyzing Group A (Celebrity A)...\")\n",
        "scores_group_A = analyze_group(group_A_embeddings)\n",
        "\n",
        "print(\"Analyzing Group B (Celebrity B)...\")\n",
        "scores_group_B = analyze_group(group_B_embeddings)\n",
        "\n",
        "print(\"Analyzing Group C (Mixed Control)...\")\n",
        "scores_group_C = analyze_group(group_C_embeddings)\n",
        "\n",
        "# --- Step 3: Present the Results ---\n",
        "print(\"\\n--- NUMERICAL CONSISTENCY COMPARISON (Mean ± Standard Deviation) ---\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"| Heuristic       | Group A (Intra-ID)       | Group B (Intra-ID)       | Group C (Inter-ID)       |\")\n",
        "print(\"-\" * 70)\n",
        "for key in scores_group_A:\n",
        "    mean_a, std_a = np.mean(scores_group_A[key]), np.std(scores_group_A[key])\n",
        "    mean_b, std_b = np.mean(scores_group_B[key]), np.std(scores_group_B[key])\n",
        "    mean_c, std_c = np.mean(scores_group_C[key]), np.std(scores_group_C[key])\n",
        "    print(f\"| {key:<15} | {mean_a:8.2f} ± {std_a:7.2f} | {mean_b:8.2f} ± {std_b:7.2f} | {mean_c:8.2f} ± {std_c:7.2f} |\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "from scipy.stats import levene\n",
        "\n",
        "# Comparing the variances of Originality\n",
        "stat, p_value_B_vs_C = levene(scores_group_B['Originality'], scores_group_C['Originality'])\n",
        "\n",
        "print(f\"\\nLevene's Test for Variances (Originality): Group B vs. Group C\")\n",
        "print(f\"P-value: {p_value_B_vs_C:.4f}\")\n",
        "\n",
        "if p_value_B_vs_C < 0.05:\n",
        "    print(\"Conclusion: The difference in variances is statistically significant.\")\n",
        "else:\n",
        "    print(\"Conclusion: There is no statistical evidence that the variances are different.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9dYcTne-Wbw"
      },
      "source": [
        " Non-parametric distribution tests (Mann-Whitney U and Kolmogorov-Smirnov) to compare the distributions of Originality scores between two groups (presumably Celebrity A and Celebrity B, based on the variable names). It also includes a function for bootstrap analysis to estimate confidence intervals for the difference in variances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u6j7BcjnKZB"
      },
      "outputs": [],
      "source": [
        "#Non-Parametric Distribution Tests\n",
        "#Once you have your 50+ images per identity, you compare the distributions of Originality (or Uniqueness) of A vs B\n",
        "from scipy.stats import mannwhitneyu, ks_2samp\n",
        "\n",
        "# Assuming scores_grupo_A['Originalidade'] and scores_grupo_B['Originalidade'] are numpy arrays of Originality\n",
        "u_stat, p_mw = mannwhitneyu(scores_group_A['Originality'], scores_group_B['Originality'], alternative='two-sided')\n",
        "ks_stat, p_ks = ks_2samp(scores_group_A['Originality'], scores_group_B['Originality'])\n",
        "\n",
        "print(f\"Mann-Whitney U: stat={u_stat:.1f}, p={p_mw:.3f}\")\n",
        "print(f\"Kolmogorov-Smirnov: stat={ks_stat:.3f}, p={p_ks:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL828uf5-0Sd"
      },
      "source": [
        "Function for performing bootstrap analysis to estimate confidence intervals for a statistic (e.g., variance difference) between two datasets. It then attempts to use this function to calculate the 95% confidence interval for the difference in variances of Originality scores between two groups (again, presumably Celebrity A and Celebrity B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfyi7C9Fn8Xl"
      },
      "outputs": [],
      "source": [
        "#Bootstrap to Estimate Confidence Intervals\n",
        "import numpy as np\n",
        "\n",
        "def bootstrap_statistic(data1, data2, stat_fn, n_boot=1000):\n",
        "    \"\"\"\n",
        "    Returns the bootstrap distribution of stat_fn(data1) - stat_fn(data2).\n",
        "    \"\"\"\n",
        "    diffs = []\n",
        "    combined = np.concatenate([data1, data2])\n",
        "    n1, n2 = len(data1), len(data2)\n",
        "    for _ in range(n_boot):\n",
        "        # Resampling with replacement\n",
        "        b1 = np.random.choice(data1, size=n1, replace=True)\n",
        "        b2 = np.random.choice(data2, size=n2, replace=True)\n",
        "        diffs.append(stat_fn(b1) - stat_fn(b2))\n",
        "    return np.array(diffs)\n",
        "\n",
        "# Example: difference of variances of Originality\n",
        "boot_diffs = bootstrap_statistic(scores_group_A['Originality'], scores_group_B['Originality'], np.var, n_boot=2000)\n",
        "ci_lower, ci_upper = np.percentile(boot_diffs, [2.5, 97.5])\n",
        "print(f\"95% CI of variance difference: [{ci_lower:.2f}, {ci_upper:.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DcZ85pO_JaP"
      },
      "source": [
        " Visualization groups of images belonging to the selected celebrities (Celebrity 1 and Celebrity 2). It plots a fixed grid of images (up to 15 per group) to provide a visual comparison of the individuals within each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCsVftM8i6gR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- Requirements ---\n",
        "# Ensure these variables exist from previous cells:\n",
        "# - celebrity_A_images: The list of 15 filenames for the first celebrity.\n",
        "# - celebrity_B_images: The list of 15 filenames for the second celebrity.\n",
        "# - base_image_path: The variable with the path to the image folder. Ex: \"/content/data/\"\n",
        "\n",
        "print(\"--- Starting Identity Group Visualization ---\")\n",
        "\n",
        "# Helper function to plot a group of images\n",
        "def plot_group(file_list, title):\n",
        "    # Plot only the first 15 images to fit in the 3x5 grid\n",
        "    images_to_plot = file_list[:15] # <--- Modification here\n",
        "    num_images = len(images_to_plot)\n",
        "\n",
        "    plt.figure(figsize=(15, 9))\n",
        "    plt.suptitle(title, fontsize=20)\n",
        "\n",
        "    # Define the subplot grid (3 rows, 5 columns)\n",
        "    cols = 5\n",
        "    rows = 3 # <--- Modification here to fix at 3x5\n",
        "    # Adjust the number of axes for the fixed grid, even if we have less than 15 images\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
        "\n",
        "    # Flatten the 2D grid into a 1D array and select only the axes that will be used\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, filename in enumerate(images_to_plot): # <--- Iterate over the sliced list\n",
        "        # Create a subplot in the corresponding position (from 1 to 15)\n",
        "        ax = axes[i] # <--- Access the axis by index in the flattened array\n",
        "\n",
        "        # full_path = os.path.join(base_image_path, filename) # Original line, replaced below\n",
        "\n",
        "        full_path = os.path.join(\"/content/data/img_align_celeba/img_align_celeba\", filename) # <--- Adjusted path\n",
        "\n",
        "        try:\n",
        "            img = Image.open(full_path)\n",
        "            ax.imshow(img)\n",
        "        except FileNotFoundError:\n",
        "            ax.text(0.5, 0.5, 'Image not found', ha='center')\n",
        "\n",
        "        ax.set_title(f\"Image {i+1}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide the subplots that were not used (only for the case where we have less than 15 images)\n",
        "    for j in range(num_images, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# Plot Group A\n",
        "print(\"\\nGroup A (Blue -'):\")\n",
        "plot_group(celebrity_A_images, \"Group A: Celebrity 1\")\n",
        "\n",
        "# Plot Group B\n",
        "print(\"\\nGroup B (Red -):\")\n",
        "plot_group(celebrity_B_images, \"Group B: Celebrity 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmd8l5Fj_kO7"
      },
      "source": [
        " UMAP dimensionality reduction followed by HDBSCAN clustering on the real data latent space embeddings. It installs the `umap-learn` library, scales the embeddings, applies UMAP to create a 2D representation, and then applies HDBSCAN to find clusters in this 2D space. Finally, it visualizes the resulting clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7XmMNOdB78g"
      },
      "outputs": [],
      "source": [
        "# --- Step 1.A: Install necessary libraries ---\n",
        "!pip install umap-learn\n",
        "\n",
        "import umap\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Requirements ---\n",
        "# Ensure the variable 'real_embeddings_numpy' with the 10,000 embeddings exists.\n",
        "\n",
        "print(\"--- Starting UMAP + HDBSCAN Analysis ---\")\n",
        "\n",
        "# --- Step 1.B: Preprocess the data ---\n",
        "# Standardizing data is crucial for UMAP\n",
        "scaler = StandardScaler()\n",
        "embeddings_scaled = scaler.fit_transform(real_embeddings_numpy)\n",
        "\n",
        "# --- Step 1.C: Apply UMAP to reduce dimensionality ---\n",
        "print(\"\\nApplying UMAP to create a 2D representation...\")\n",
        "reducer = umap.UMAP(\n",
        "    n_neighbors=15,  # Controls the balance between local and global structure\n",
        "    min_dist=0.1,    # Controls how \"together\" points in a cluster stay\n",
        "    n_components=2,  # We want a 2D map\n",
        "    metric='euclidean',\n",
        "    random_state=42\n",
        ")\n",
        "embedding_2d = reducer.fit_transform(embeddings_scaled)\n",
        "print(\"UMAP 2D representation created successfully.\")\n",
        "\n",
        "# --- Step 1.D: Apply HDBSCAN to the new representation ---\n",
        "print(\"\\nApplying HDBSCAN to the UMAP representation...\")\n",
        "clusterer = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=100, # We start with a high value to find large structures\n",
        "    min_samples=10\n",
        ")\n",
        "clusters_umap = clusterer.fit_predict(embedding_2d)\n",
        "\n",
        "n_clusters = len(set(clusters_umap)) - (1 if -1 in clusters_umap else 0)\n",
        "n_noise = list(clusters_umap).count(-1)\n",
        "print(f\"HDBSCAN found {n_clusters} cluster(s) and {n_noise} noise point(s).\")\n",
        "\n",
        "# --- Step 1.E: Visualize the result ---\n",
        "plt.figure(figsize=(12, 10))\n",
        "# Plot the \"noise\" points (label -1) in light gray\n",
        "plt.scatter(embedding_2d[clusters_umap == -1, 0], embedding_2d[clusters_umap == -1, 1], c=(0.8, 0.8, 0.8), s=1, alpha=0.5)\n",
        "# Plot the found clusters with different colors\n",
        "plt.scatter(embedding_2d[clusters_umap != -1, 0], embedding_2d[clusters_umap != -1, 1], c=clusters_umap[clusters_umap != -1], s=5, cmap='viridis')\n",
        "plt.title('Clusters found by HDBSCAN in UMAP representation', fontsize=16)\n",
        "plt.xlabel('UMAP 1 Dimension')\n",
        "plt.ylabel('UMAP 2 Dimension')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVs_jBNNABJX"
      },
      "source": [
        " Checkpoint to save the processed attributes and analysis results (cluster labels, Uniqueness, and Originality scores) into a Parquet file. It ensures that the necessary variables exist and are aligned before saving the data, allowing you to resume the analysis from this point later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW_XLbAxBNw1"
      },
      "outputs": [],
      "source": [
        "#=============================\n",
        "# CHECKPOINT CREATION CELL\n",
        "# Execute this cell and, if everything goes well, RESTART THE RUNTIME ENVIRONMENT.\n",
        "# ==========================================================\n",
        "print(\"Starting checkpoint creation...\")\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms # Import transforms here too\n",
        "import numpy as np # Import numpy here\n",
        "\n",
        "\n",
        "base_dataset_path = '/content/data'\n",
        "\n",
        "try:\n",
        "\n",
        "    print(\"Checking for existing variables for dataset, subset indices, and clusters...\")\n",
        "    _ = full_dataset\n",
        "    _ = subset_indices\n",
        "    _ = clusters_umap # Verify if clusters_umap exists\n",
        "    _ = uniqueness_scores # Verify uniqueness_scores exists\n",
        "    _ = originality_scores # Verify originality_scores exists\n",
        "    print(\"Variables full_dataset, subset_indices, clusters_umap, uniqueness_scores, and originality_scores found.\")\n",
        "except NameError:\n",
        "    print(\"One or more variables (full_dataset, subset_indices, clusters_umap, uniqueness_scores, originality_scores) not found. Recreating necessary ones...\")\n",
        "    # Define basic transformations to load the dataset (not used for attributes, but needed for ImageFolder)\n",
        "    basic_transforms = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
        "    full_dataset = datasets.ImageFolder(root=base_dataset_path, transform=basic_transforms)\n",
        "    # Recreate subset_indices (assuming 10000 samples, adjust if needed)\n",
        "    subset_indices = list(range(10000))\n",
        "    print(\"full_dataset and subset_indices recreated. Note that clusters_umap, uniqueness_scores and originality_scores are *not* recreated here and will need to be generated by the relevant cells.\")\n",
        "    # Define clusters_umap, uniqueness_scores, originality_scores as None to detect they are not available\n",
        "    clusters_umap = None\n",
        "    uniqueness_scores = None\n",
        "    originality_scores = None\n",
        "\n",
        "\n",
        "# --- Load and preprocess attributes (Moved from the previous cell to ensure it exists) ---\n",
        "attributes_path = \"data/list_attr_celeba.csv\"\n",
        "subset_attributes = pd.DataFrame() # Initialize as empty\n",
        "try:\n",
        "    df_attributes = pd.read_csv(attributes_path).set_index('image_id')\n",
        "    print(\"Attributes file loaded successfully.\")\n",
        "\n",
        "    # Get file names for the subset\n",
        "    # Adjust the base path if necessary to match the actual structure in your file system\n",
        "    # ImageFolder returns full paths, so we need to extract just the file name\n",
        "    # We use the subset indices in the full_dataset to get the original paths and extract the file names\n",
        "    file_names = [os.path.basename(full_dataset.samples[i][0]) for i in subset_indices]\n",
        "\n",
        "    # Filter the attributes DataFrame using the subset file names\n",
        "    # Use .loc for label-based indexing (file names)\n",
        "    # Check if all subset file names exist in the df_attributes index\n",
        "    existing_files = [name for name in file_names if name in df_attributes.index]\n",
        "    if len(existing_files) != len(file_names):\n",
        "        print(f\"Warning: {len(file_names) - len(existing_files)} subset file names were not found in the attributes file.\")\n",
        "        # Filter file_names to include only those that exist in the index\n",
        "        file_names = existing_files\n",
        "\n",
        "    if file_names: # Proceed only if there are existing files to process\n",
        "        subset_attributes = df_attributes.loc[file_names].copy()\n",
        "\n",
        "        # Convert attributes from -1/1 to 0/1\n",
        "        subset_attributes = (subset_attributes + 1) / 2\n",
        "        print(f\"Attributes DataFrame for the subset created successfully: {subset_attributes.shape}\")\n",
        "    else:\n",
        "        print(\"No subset file names found in the attributes file. Could not create subset_attributes.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The attributes file was not found at '{attributes_attributes}'.\")\n",
        "    print(\"Please check the path and ensure the .csv file is accessible.\")\n",
        "except KeyError as e:\n",
        "    print(f\"ERROR: Some subset file name was not found in the attributes file during loc: {e}\")\n",
        "    print(\"Please check if the file names in the subset match the IDs in the attributes file.\")\n",
        "\n",
        "\n",
        "# 1. We take the attributes you have already processed (values 0 or 1)\n",
        "# The variable should be called 'subset_attributes' as per your code.\n",
        "# We check if subset_attributes was created successfully before proceeding\n",
        "if not subset_attributes.empty:\n",
        "    df_to_save = subset_attributes.copy()\n",
        "\n",
        "    # 2. Add cluster labels, Uniqueness, and Originality as new columns\n",
        "    # Check if clusters_umap, uniqueness_scores and originality_scores exist and are not None\n",
        "    if clusters_umap is not None and uniqueness_scores is not None and originality_scores is not None:\n",
        "        # Ensure the number of labels/scores matches the number of samples in subset_attributes\n",
        "        if len(clusters_umap) == len(df_to_save) and len(uniqueness_scores) == len(df_to_save) and len(originality_scores) == len(df_to_save):\n",
        "            df_to_save['cluster_label'] = clusters_umap\n",
        "            df_to_save['Unicidade'] = uniqueness_scores # Add Unicidade score\n",
        "            df_to_save['Originalidade'] = originality_scores # Add Originalidade score\n",
        "\n",
        "            # 3. We will save in Parquet format, which is fast and efficient.\n",
        "            file_path = 'checkpoint_final_results.parquet'\n",
        "            df_to_save.to_parquet(file_path)\n",
        "\n",
        "            print(\"--- SUCCESS! ---\")\n",
        "            print(f\"Checkpoint with {df_to_save.shape[0]} rows and {df_to_save.shape[1]} columns saved to: '{file_path}'\")\n",
        "            print(\"The DataFrame contains attributes, 'cluster_label', 'Unicidade' and 'Originalidade'.\")\n",
        "            print(\"You can safely restart the runtime environment now.\")\n",
        "            print(\"==========================================================\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nERROR: The number of labels/scores does not match the number of samples in the subset attributes DataFrame.\")\n",
        "            print(f\"Number of labels (clusters_umap): {len(clusters_umap) if clusters_umap is not None else 'N/A'}\")\n",
        "            print(f\"Number of scores (uniqueness_scores): {len(uniqueness_scores) if uniqueness_scores is not None else 'N/A'}\")\n",
        "            print(f\"Number of scores (originality_scores): {len(originality_scores) if originality_scores is not None else 'N/A'}\")\n",
        "            print(f\"Number of samples in subset_attributes: {len(df_to_save)}\")\n",
        "            print(\"Please run the data loading cells and the analysis cells (including UMAP+HDBSCAN and heuristics) to ensure all data is synchronized before saving the checkpoint.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nERROR: One or more variables ('clusters_umap', 'uniqueness_scores', 'originality_scores') are not defined or are None.\")\n",
        "        print(\"Please run the relevant cells to generate these variables before executing this checkpoint cell.\")\n",
        "else:\n",
        "    print(\"\\nCould not create the 'subset_attributes' DataFrame. Checkpoint was not saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klNMsR2cCD_G"
      },
      "source": [
        " Function `compute_abi_scores` to calculate the Attribute Bias Index (ABI) for given attributes based on a mask (indicating a specific cluster or group). It then iterates through the unique clusters found by HDBSCAN, calculates the ABI scores for each cluster, visualizes the top 10 attributes with the highest Odds Ratios for each cluster using bar plots, and displays sample images from each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLNnNVXhRXfO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def compute_abi_scores(attributes_df, mask, eps=1e-6):\n",
        "    results = []\n",
        "    cluster_mask = mask\n",
        "    rest_mask = ~mask\n",
        "\n",
        "    # Handle cases where one of the masks is empty to avoid division by zero\n",
        "    if cluster_mask.sum() == 0 or rest_mask.sum() == 0:\n",
        "        print(\"Warning: One of the masks is empty. Cannot compute meaningful ABI scores.\")\n",
        "        return pd.DataFrame({\"Attribute\": attributes_df.columns, \"Odds Ratio\": np.nan, \"Interpretation\": \"N/A\"})\n",
        "\n",
        "\n",
        "    for col in attributes_df.columns:\n",
        "        # Ensure we are using .loc with the boolean mask for correct indexing\n",
        "        A = attributes_df.loc[cluster_mask, col].sum() + eps\n",
        "        B = cluster_mask.sum() - (A - eps) + eps # Number of samples *not* having the attribute in the cluster\n",
        "        C = attributes_df.loc[rest_mask, col].sum() + eps\n",
        "        D = rest_mask.sum() - (C - eps) + eps # Number of samples *not* having the attribute outside the cluster\n",
        "\n",
        "\n",
        "        odds_cluster = A / B\n",
        "        odds_rest = C / D\n",
        "\n",
        "        # Handle cases where odds_rest is zero to avoid division by zero\n",
        "        if odds_rest == 0:\n",
        "            odds_ratio = np.inf if odds_cluster > 0 else 1.0 # Infinite if cluster has the attribute, 1.0 otherwise\n",
        "        else:\n",
        "            odds_ratio = odds_cluster / odds_rest\n",
        "\n",
        "\n",
        "        if odds_ratio > 1.15:\n",
        "            interp = \"Over-represented\"\n",
        "        elif odds_ratio < 0.85:\n",
        "            interp = \"Under-represented\"\n",
        "        else:\n",
        "            interp = \"Neutral\"\n",
        "\n",
        "        results.append({\n",
        "            \"Attribute\": col,\n",
        "            \"Odds Ratio\": round(odds_ratio, 2),\n",
        "            \"Interpretation\": interp\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).sort_values(\"Odds Ratio\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Adjust these paths and variables according to your environment:\n",
        "attributes_path = \"data/list_attr_celeba.csv\"\n",
        "#full_dataset = ...  # your loaded CelebA dataset\n",
        "#subset_indices = ...  # indices of the 10k images\n",
        "#clusters_umap = ...  # HDBSCAN labels after UMAP\n",
        "\n",
        "# Load and preprocess attributes\n",
        "df_attributes = pd.read_csv(attributes_path).set_index('image_id')\n",
        "file_names = [os.path.basename(full_dataset.samples[i][0]) for i in subset_indices]\n",
        "subset_attributes = df_attributes.loc[file_names]\n",
        "subset_attributes = (subset_attributes + 1) / 2  # from -1/1 to 0/1\n",
        "\n",
        "# Analysis by cluster\n",
        "unique_labels = [l for l in np.unique(clusters_umap) if l != -1]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define a color palette for the clusters\n",
        "palette = sns.color_palette(\"tab10\", n_colors=len(unique_labels))\n",
        "label2color = {lbl: palette[i] for i, lbl in enumerate(unique_labels)}\n",
        "\n",
        "print(\"--- CLUSTER SUMMARY ---\")\n",
        "for lbl in unique_labels:\n",
        "    count = (clusters_umap == lbl).sum()\n",
        "    color = label2color[lbl]\n",
        "for label in unique_labels:\n",
        "    print(f\"\\n--- CLUSTER {label} ---\")\n",
        "    print(f\"Number of samples in the cluster: {np.sum(clusters_umap == label)}\")\n",
        "    idxs = np.where(clusters_umap == label)[0]\n",
        "    print(f\"SBS = {len(idxs)/len(clusters_umap):.4f}\")\n",
        "\n",
        "    # Use the full-length mask for the cluster\n",
        "    full_length_mask_cluster = np.zeros(len(subset_attributes), dtype=bool)\n",
        "    full_length_mask_cluster[idxs] = True\n",
        "\n",
        "    abi_df = compute_abi_scores(subset_attributes, full_length_mask_cluster)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.barplot(\n",
        "    data=abi_df.head(10),\n",
        "    x=\"Odds Ratio\", y=\"Attribute\",\n",
        "    color=label2color[label]\n",
        "                           )\n",
        "    plt.title(f\"ABI Top 10 — Cluster {label}\")\n",
        "    plt.axvline(1.0, ls='--', color='gray')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    display(abi_df.head(10))\n",
        "\n",
        "    # Sample visualization\n",
        "    sample_idxs = np.random.choice(idxs, size=min(100, len(idxs)), replace=False)\n",
        "    cols = 5\n",
        "    rows = int(np.ceil(len(sample_idxs) / cols))\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
        "    axes = axes.flatten()\n",
        "    for ax in axes[len(sample_idxs):]:\n",
        "        ax.axis('off')\n",
        "    for i, idx in enumerate(sample_idxs):\n",
        "        path = os.path.join(\"/content/data/img_align_celeba/img_align_celeba\", file_names[idx])\n",
        "        axes[i].imshow(Image.open(path))\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(f\"Cluster {label} Samples\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B7W5gd-CbJf"
      },
      "source": [
        " Analysis of the \"Creative Frontier\" (outliers) by calculating the Creative Latent Score (CLS) for each sample. It normalizes the Uniqueness and Originality scores and combines them to get the CLS. It then identifies and visualizes the top N samples with the highest CLS as potential \"black swans\" or creative outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "untvGkgro8Kv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Requirements ---\n",
        "# Ensure these variables exist:\n",
        "# - uniqueness_scores: The array with Uniqueness scores for the 10,000 samples.\n",
        "# - originality_scores: The array with Originality scores.\n",
        "# - subset_indices: The indices of the 10,000 images we used.\n",
        "# - full_dataset: The complete CelebA dataset.\n",
        "\n",
        "print(\"--- STARTING CREATIVE FRONTIER ANALYSIS (OUTLIERS) ---\")\n",
        "\n",
        "# --- Step 1: Calculate the Creative Latent Score (CLS) ---\n",
        "# We normalize the scores to the [0, 1] scale so they can be combined fairly.\n",
        "scaler = MinMaxScaler()\n",
        "unicidade_norm = scaler.fit_transform(uniqueness_scores.reshape(-1, 1))\n",
        "originalidade_norm = scaler.fit_transform(originality_scores.reshape(-1, 1))\n",
        "\n",
        "# Define alpha (the weight) and calculate CLS\n",
        "alpha = 0.5\n",
        "cls_scores = alpha * unicidade_norm + (1 - alpha) * originalidade_norm\n",
        "cls_scores = cls_scores.flatten() # Flatten back to a 1D array\n",
        "\n",
        "# ---\n",
        "# --- Step 2: Find the Top N Candidates ---\n",
        "# Sort the sample indices based on their CLS, from highest to lowest.\n",
        "top_n_indices = np.argsort(cls_scores)[::-1]\n",
        "\n",
        "# Select the top 150\n",
        "top_150_creative = top_n_indices[:150]\n",
        "\n",
        "print(f\"Top 150 most 'creative' samples (highest CLS) found.\")\n",
        "\n",
        "# --- Step 3: Visualize the \"Black Swans\" ---\n",
        "print(\"\\nVisualizing samples with the highest Creative Latent Score (CLS):\")\n",
        "\n",
        "# Get the file names from our subset\n",
        "file_names = [os.path.basename(full_dataset.samples[i][0]) for i in subset_indices]\n",
        "\n",
        "# Plot the images\n",
        "base_image_path = \"/content/data/img_align_celeba/img_align_celeba\"\n",
        "num_images = len(top_150_creative)\n",
        "\n",
        "\n",
        "cols = 5\n",
        "rows = int(np.ceil(num_images / cols))\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, idx in enumerate(top_150_creative):\n",
        "    file_name = file_names[idx]\n",
        "    image_path = os.path.join(base_image_path, file_name)\n",
        "    img = Image.open(image_path)\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"Ranking {i+1} \\nId: {idx}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "\n",
        "for j in range(i + 1, len(axes)): axes[j].axis('off')\n",
        "plt.suptitle(\"Top 150 Samples by Creative Latent Score (CLS)\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEReAOr3Ch85"
      },
      "source": [
        "This cell analyzes \"Stereotypes\" by identifying samples with the lowest Originality scores. It finds the top N samples with the lowest originality scores and visualizes these images as representatives of the most generic or stereotypical samples in the latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5r_7b63sKL3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# --- Requirements ---\n",
        "# Ensure these variables exist:\n",
        "# - originality_scores: The array with Originality scores for the 10,000 samples.\n",
        "# - subset_indices: The indices of the 10,000 images we used.\n",
        "# - full_dataset: The complete CelebA dataset.\n",
        "\n",
        "print(\"--- STEREOTYPE ANALYSIS (LOWEST SCORES) ---\")\n",
        "\n",
        "# --- Step 1: Find the Top N Candidates with LOWEST Originality ---\n",
        "# Sort the sample indices based on their score, from LOWEST to highest.\n",
        "bottom_n_indices = np.argsort(originality_scores)\n",
        "\n",
        "# Select the bottom 40 (the most generic)\n",
        "top_140_generic = bottom_n_indices[:140]\n",
        "\n",
        "print(f\"Top 140 most 'generic' samples (lowest Originality) found.\")\n",
        "\n",
        "# --- Step 2: Visualize the Stereotypes ---\n",
        "print(\"\\nVisualizing samples with the lowest Entropic Originality Score:\")\n",
        "\n",
        "# Get the file names from our subset\n",
        "file_names = [os.path.basename(full_dataset.samples[i][0]) for i in subset_indices]\n",
        "\n",
        "# Plot the images\n",
        "base_image_path = \"/content/data/img_align_celeba/img_align_celeba\"\n",
        "num_images = len(top_140_generic)\n",
        "cols = 5\n",
        "rows = int(np.ceil(num_images / cols))\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, idx in enumerate(top_140_generic):\n",
        "    file_name = file_names[idx]\n",
        "    image_path = os.path.join(base_image_path, file_name)\n",
        "    img = Image.open(image_path)\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"Generic Rank {i+1}\\nIndex: {idx}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "for j in range(i + 1, len(axes)): axes[j].axis('off')\n",
        "plt.suptitle(\"Top 140 Samples by Lowest Originality Score (Stereotypes)\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDmCOkYvDOFD"
      },
      "source": [
        "This is a quantitative bias analysis (SBS & ABI). It identifies the stereotypical cluster (C_max) by finding the largest cluster in the raw embeddings using HDBSCAN. It calculates the Stereotypical Bias Score (SBS) as the proportion of samples in this cluster. It then calculates the Attribute Bias Index (ABI) for selected attributes to assess how super or sub-represented they are within the stereotypical cluster compared to the overall dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E-FFDMBXap0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hdbscan\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"--- STARTING QUANTITATIVE BIAS ANALYSIS (SBS & ABI) ---\")\n",
        "\n",
        "# --- Step 1: Identify the Stereotypical Cluster (C_max) ---\n",
        "print(\"\\n1. Identifying the central cluster with HDBSCAN on raw embeddings...\")\n",
        "scaler_bias = StandardScaler()\n",
        "embeddings_scaled_bias = scaler_bias.fit_transform(real_embeddings_numpy)\n",
        "\n",
        "clusterer_bias = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=2, # Using the value that worked for you\n",
        "    metric='euclidean'\n",
        ")\n",
        "clusters_bias = clusterer_bias.fit_predict(embeddings_scaled_bias)\n",
        "\n",
        "labels, counts = np.unique(clusters_bias[clusters_bias!=-1], return_counts=True)\n",
        "if len(counts) > 0:\n",
        "    c_max_label = labels[np.argmax(counts)]\n",
        "    indices_c_max = np.where(clusters_bias == c_max_label)[0]\n",
        "    print(f\"Stereotypical cluster (C_max) found with label {c_max_label}, containing {len(indices_c_max)} samples.\")\n",
        "else:\n",
        "    print(\"No significant cluster found. Bias analysis cannot proceed.\")\n",
        "    indices_c_max = []\n",
        "\n",
        "# --- Step 2: Calculate the Stereotypical Bias Score (SBS) ---\n",
        "if len(indices_c_max) > 0:\n",
        "    N = len(real_embeddings_numpy)\n",
        "    sbs_score = len(indices_c_max) / N\n",
        "    print(f\"\\n2. Stereotypical Bias Score (SBS): {sbs_score:.4f} ({sbs_score:.2%})\")\n",
        "    print(f\"   -> {sbs_score:.2%} of the dataset belongs to the main small hyper-dense yellow cluster.\")\n",
        "\n",
        "    # --- Step 3: Calculate the Attribute Bias Index (ABI) ---\n",
        "    print(\"\\n3. Calculating the Attribute Bias Index (ABI) for selected attributes...\")\n",
        "\n",
        "    # Prepare the attributes DataFrame\n",
        "    file_names = [os.path.basename(full_dataset.samples[i][0]) for i in subset_indices]\n",
        "\n",
        "    # APPLY THE ISIN FILTER ON THE ORIGINAL DATAFRAME BEFORE INDEXING\n",
        "    subset_attributes = df_attributes[df_attributes.index.isin(file_names)].copy()\n",
        "\n",
        "    # Check if the filtered DataFrame is not empty\n",
        "    if subset_attributes.empty:\n",
        "        print(\"Error: The subset attributes DataFrame is empty after filtering. Check file names.\")\n",
        "    else:\n",
        "        # Convert attributes to 0 and 1 for the mean calculation\n",
        "        subset_attributes_norm = (subset_attributes + 1) / 2\n",
        "\n",
        "        attributes_for_analysis = ['Smiling', 'Male', 'Young', 'Blond_Hair', 'Eyeglasses', 'No_Beard']\n",
        "\n",
        "        # Ensure selected attributes exist in the DataFrame\n",
        "        available_attributes = [attr for attr in attributes_for_analysis if attr in subset_attributes_norm.columns]\n",
        "        if len(available_attributes) != len(attributes_for_analysis):\n",
        "            missing = [attr for attr in attributes_for_analysis if attr not in subset_attributes_norm.columns]\n",
        "            print(f\"Warning: The following attributes were not found in the dataset and will be skipped: {missing}\")\n",
        "        attributes_for_analysis = available_attributes # Update list to only include available attributes\n",
        "\n",
        "\n",
        "        if not attributes_for_analysis:\n",
        "            print(\"No valid attributes selected for ABI analysis.\")\n",
        "        else:\n",
        "            general_prob = subset_attributes_norm[attributes_for_analysis].mean()\n",
        "\n",
        "            # Isolate attributes only from our C_max cluster\n",
        "            c_max_file_names = [file_names[i] for i in indices_c_max]\n",
        "\n",
        "            # Use the already indexed DataFrame for the lookup\n",
        "            # The correction here is that subset_attributes_norm ALREADY HAS image_id as index\n",
        "            attributes_c_max = subset_attributes_norm.loc[c_max_file_names]\n",
        "            prob_in_cluster = attributes_c_max[attributes_for_analysis].mean()\n",
        "\n",
        "            # Add a small constant to avoid division by zero if general probability is 0\n",
        "            abi_scores = prob_in_cluster / (general_prob + 1e-8)\n",
        "\n",
        "\n",
        "            print(\"\\nABI Results (Attribute Odds Ratio in Stereotype vs. Dataset):\")\n",
        "            print(\"-\" * 50)\n",
        "            print(f\"| Attribute         | ABI Score | Interpretation\")\n",
        "            print(\"-\" * 50)\n",
        "            for attr, score in abi_scores.items():\n",
        "                interpretation = \"Over-represented\" if score > 1.1 else \"Under-represented\" if score < 0.9 else \"Neutral representation\"\n",
        "                print(f\"| {attr:<17} | {score:9.2f} | {interpretation}\")\n",
        "            print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSkxCQ6DDpgS"
      },
      "source": [
        "This cell generates the final visualizations for the research paper. It creates a bar chart showing the Attribute Bias Index (ABI) for selected attributes in the stereotypical cluster, highlighting super-represented, sub-represented, and neutral attributes. It also generates a scatter plot of normalized Uniqueness versus Originality scores, colored by the HDBSCAN cluster labels, to visualize the heuristic space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBaNdS92eZ_L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Requirements ---\n",
        "# Ensure these variables exist from the previous cells:\n",
        "# - abi_scores: The pandas Series with the Attribute Bias Index results.\n",
        "# - uniqueness_scores: The array with Uniqueness scores for the 10,000 samples.\n",
        "# - originality_scores: The array with Originality scores.\n",
        "# - clusters_umap: The array with the cluster labels from the UMAP + HDBSCAN analysis.\n",
        "# - embedding_2d: The 2D representation generated by UMAP.\n",
        "\n",
        "print(\"--- GENERATING FINAL RESEARCH VISUALIZATIONS ---\")\n",
        "\n",
        "# --- Plot 1: Attribute Bias Index (ABI) Bar Chart ---\n",
        "\n",
        "print(\"\\nGenerating Plot 1: Attribute Bias Analysis (ABI)...\")\n",
        "\n",
        "# Prepare data for the plot\n",
        "abi_to_plot = abi_scores.sort_values()\n",
        "colors = ['red' if x < 0.9 else 'green' if x > 1.1 else 'grey' for x in abi_to_plot]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "bars = plt.barh(abi_to_plot.index, abi_to_plot.values, color=colors)\n",
        "\n",
        "# Add a vertical line at the neutral point (ABI = 1.0)\n",
        "plt.axvline(1.0, color='black', linestyle='--', linewidth=1)\n",
        "plt.xlabel('Attribute Bias Index (ABI Score)', fontsize=12)\n",
        "plt.title('Attribute Bias in the Stereotypical Cluster (C_max)', fontsize=16)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add the value of each bar for clarity\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "             f'{bar.get_width():.2f}',\n",
        "             va='center', ha='left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Plot 2: Uniqueness vs. Originality Scatter Plot ---\n",
        "\n",
        "print(\"\\nGenerating Plot 2: Heuristic Space Map (Uniqueness vs. Originality)...\")\n",
        "\n",
        "# Normalize the scores to the [0, 1] scale for fair visualization\n",
        "scaler = MinMaxScaler()\n",
        "unicidade_norm = scaler.fit_transform(uniqueness_scores.reshape(-1, 1))\n",
        "originalidade_norm = scaler.fit_transform(originality_scores.reshape(-1, 1))\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Plot the \"noise\" points (label -1) first, in light gray\n",
        "noise_indices = clusters_umap == -1\n",
        "plt.scatter(unicidade_norm[noise_indices], originalidade_norm[noise_indices],\n",
        "            c='lightgray', s=5, alpha=0.5, label='Noise (Unclustered)')\n",
        "\n",
        "# Plot the found clusters with different colors\n",
        "cluster_indices = clusters_umap != -1\n",
        "scatter = plt.scatter(unicidade_norm[cluster_indices], originalidade_norm[cluster_indices],\n",
        "                      c=clusters_umap[cluster_indices], s=15, cmap='viridis',\n",
        "                      label='Identified Clusters')\n",
        "\n",
        "\n",
        "plt.title('Heuristic Map: Uniqueness vs. Originality', fontsize=18)\n",
        "plt.xlabel('Uniqueness Score (Normalized)', fontsize=14)\n",
        "plt.ylabel('Originality Score (Normalized)', fontsize=14)\n",
        "plt.legend()\n",
        "plt.colorbar(scatter, label='Cluster ID (HDBSCAN)')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVLorrT4ECAA"
      },
      "source": [
        " Calculation of  Spearman correlation coefficient between the Uniqueness and Originality scores to assess their relationship. It then plots a scatter plot of Uniqueness versus Originality scores with a diagonal arrow indicating the direction of increasing scores, providing a visual representation of the heuristic space and the correlation between the two metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhRO2aNFdPVq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Load the scores from the checkpoint DataFrame\n",
        "try:\n",
        "    df_results = pd.read_parquet('checkpoint_final_results.parquet')\n",
        "    print(\"Checkpoint file loaded.\")\n",
        "    # Ensure scores are accessible by original index\n",
        "    # Assuming subset_indices is available from previous cells\n",
        "    # If subset_indices is not available, this part will need adjustment\n",
        "    unicidade_scores = df_results['Unicidade'].values\n",
        "    originalidade_scores = df_results['Originalidade'].values\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Checkpoint file 'checkpoint_final_results.parquet' not found.\")\n",
        "    print(\"Please run the checkpoint saving cell first.\")\n",
        "    unicidade_scores = None\n",
        "    originalidade_scores = None\n",
        "except NameError:\n",
        "    print(\"Error: 'subset_indices' is not defined. Cannot filter scores by subset.\")\n",
        "    unicidade_scores = None\n",
        "    originalidade_scores = None\n",
        "\n",
        "\n",
        "# If unicidade_scores and originality_scores are in lists, convert:\n",
        "# Assuming they are already numpy arrays from previous steps\n",
        "if unicidade_scores is not None and originality_scores is not None:\n",
        "    uniq = np.array(unicidade_scores)\n",
        "    orig = np.array(originalidade_scores)\n",
        "\n",
        "    # 1. Calculate Spearman correlation\n",
        "    rho, pval = spearmanr(uniq, orig)\n",
        "    print(f\"Spearman ρ = {rho:.2f}, p = {pval:.3f}\")\n",
        "\n",
        "    # 2. Plot the scatter with diagonal arrow\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.scatter(uniq, orig, s=10, alpha=0.6)\n",
        "    plt.xlabel('Uniqueness Score') # Removed Normalizado as scores might not be normalized here\n",
        "    plt.ylabel('Originality Score') # Removed Normalizado as scores might not be normalized here\n",
        "    plt.title(f'Heuristic Map: Uniqueness vs. Originality\\nSpearman ρ={rho:.2f}, p={pval:.3f}')\n",
        "\n",
        "    # Add a diagonal arrow (from low to high region)\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "    plt.annotate(\n",
        "        '',\n",
        "        xy=(xmax*0.9, ymax*0.9),\n",
        "        xytext=(xmin*1.05, ymin*1.05),\n",
        "        arrowprops=dict(arrowstyle='->', lw=2)\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Scores not available for plotting and correlation calculation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TteIVTdEpIz"
      },
      "source": [
        " Analysis results from the Parquet checkpoint file. It calculates the Creative Latent Score (CLS) for each sample if it doesn't already exist. It then filters out the noise points (cluster -1) and calculates the average CLS for each identified cluster. Finally, it identifies and prints the cluster with the highest average CLS as the \"Creative Cluster (Cluster C)\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5XcAZx7fT_m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the results from our secure checkpoint\n",
        "try:\n",
        "    df_results = pd.read_parquet('checkpoint_final_results.parquet')\n",
        "    print(\"Checkpoint file loaded successfully!\")\n",
        "    print(f\"Total samples loaded: {len(df_results)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: The file 'checkpoint_final_results.parquet' was not found.\")\n",
        "    print(\"Please ensure the saving cell was executed successfully.\")\n",
        "    # Stop execution if the file does not exist\n",
        "    # df_results = None\n",
        "\n",
        "# Ensure the dataframe was loaded before proceeding\n",
        "if 'df_results' in locals() and df_results is not None:\n",
        "\n",
        "    # QUESTION: Does your dataframe already have the 'CLS' column?\n",
        "    # If not, we need to calculate it first.\n",
        "    # Assuming yes, and that the score columns are named 'Unicidade' and 'Originalidade'\n",
        "    if 'CLS' not in df_results.columns:\n",
        "        # If the CLS column does not exist, let's create it.\n",
        "        # Adapt the names 'Unicidade' and 'Originalidade' if they are different.\n",
        "        df_results['CLS'] = (df_results['Unicidade'] + df_results['Originalidade']) / 2 # Changed from 'Uniqueness' and 'Originality' to 'Unicidade' and 'Originalidade'\n",
        "        print(\"'CLS' column calculated and added.\")\n",
        "\n",
        "    # Filter out noise (cluster -1) to not include in the mean calculation\n",
        "    df_clusters_only = df_results[df_results['cluster_label'] != -1].copy()\n",
        "\n",
        "    # Calculate the average CLS per cluster\n",
        "    avg_cls_per_cluster = df_clusters_only.groupby('cluster_label')['CLS'].mean().sort_values(ascending=False)\n",
        "\n",
        "    # Identify the cluster with the highest average CLS\n",
        "    creative_cluster_id = avg_cls_per_cluster.idxmax()\n",
        "\n",
        "    print(\"\\n--- Creative Score (CLS) Analysis by Cluster ---\")\n",
        "    print(avg_cls_per_cluster)\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(f\"\\n🏆 The Creative Cluster (Cluster C) is: CLUSTER {creative_cluster_id}\")\n",
        "    print(\"----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8d7ICHE-Wh"
      },
      "source": [
        "This cell analyzes the \"noise\" region (samples not assigned to any cluster by HDBSCAN, the grey region). It calculates the mean CLS, Uniqueness, and Originality scores for these noise points. It then identifies and prepares to display the top 10 images with the highest CLS within this noise region, considering them as potential creative outliers that didn't fit into defined clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTtB1Z2qembv"
      },
      "outputs": [],
      "source": [
        "# 1. Select samples classified as \"noise\" (label -1)\n",
        "indices_noise = np.where(clusters_umap == -1)[0]\n",
        "print(f\"Number of samples outside clusters (gray region): {len(indices_noise)}\")\n",
        "\n",
        "# 2. Calculate heuristic statistics for these points\n",
        "# Load the scores from the checkpoint DataFrame\n",
        "unicidade_scores = df_results['Unicidade'].values[subset_indices] # Get scores for the subset used for UMAP\n",
        "originalidade_scores = df_results['Originalidade'].values[subset_indices] # Get scores for the subset used for UMAP\n",
        "cls_scores = df_results['CLS'].values[subset_indices] # Get CLS scores for the subset used for UMAP\n",
        "\n",
        "# Filter scores for noise indices\n",
        "cls_noise = cls_scores[indices_noise]\n",
        "unicidade_noise = unicidade_scores[indices_noise]\n",
        "originalidade_noise = originality_scores[indices_noise]\n",
        "\n",
        "print(f\"Average CLS (Gray Region): {np.mean(cls_noise):.2f} ± {np.std(cls_noise):.2f}\")\n",
        "print(f\"Average Uniqueness: {np.mean(unicidade_noise):.2f}\")\n",
        "print(f\"Average Originality: {np.mean(originalidade_noise):.2f}\")\n",
        "\n",
        "# 3. Show the top 10 most creative images from the gray region\n",
        "top_idxs = indices_noise[np.argsort(cls_noise)[-10:]]  # Top 10 CLS in the gray region"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ujCIJPFZlu"
      },
      "source": [
        "Visualizing the top N creative outliers specifically from the \"noise\" region (samples not assigned to any cluster). It selects the top N samples with the highest CLS within the noise indices and plots their corresponding images with their CLS scores, providing a visual inspection of the most creative samples outside of the main clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9-N7vi7v6w9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Parameters\n",
        "N = 50   # number of images to plot\n",
        "base_path = \"/content/data/img_align_celeba/img_align_celeba\"\n",
        "\n",
        "# 1) Indices of the gray region\n",
        "indices_noise = np.where(clusters_umap == -1)[0]\n",
        "\n",
        "# 2) Get the N highest CLS within this region\n",
        "cls_noise = cls_scores[indices_noise]\n",
        "top_idxs_noise = indices_noise[np.argsort(cls_noise)[-N:]]\n",
        "\n",
        "# 3) Plot\n",
        "cols = 4\n",
        "rows = int(np.ceil(N/cols))\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, idx in enumerate(top_idxs_noise):\n",
        "    fname = file_names[idx]\n",
        "    img = Image.open(os.path.join(base_path, fname))\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"CLS={cls_scores[idx]:.1f}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# clean up extra axes\n",
        "for j in range(i+1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.suptitle(f\"Top {N} Creative Outliers in the Gray Region\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moYvxJFNFqgc"
      },
      "source": [
        "Exploration HDBSCAN thresholds calibration and detecting low-density \"pockets\" or \"sub-niches\" in the UMAP space. It runs HDBSCAN with different `min_cluster_size` values to see how the number of clusters changes. It then attempts to use Kernel Density Estimation (KDE) and minimum filtering to identify low-density areas (valleys) and cluster points within these valleys using DBSCAN. Finally, it calculates and displays the mean heuristic scores (CLS, Uniqueness, Originality) and ABI scores for these identified sub-niches and visualizes sample images from them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxTZkb92wOat"
      },
      "outputs": [],
      "source": [
        "#Calibrate HDBSCAN thresholds as “multi-scale”\n",
        "#Instead of a single min_cluster_size, run HDBSCAN multiple times with different values, for example:\n",
        "import hdbscan\n",
        "\n",
        "for mcs in [5, 10, 20, 50]:\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, min_samples=1, cluster_selection_epsilon=0.0)\n",
        "    labels_mcs = clusterer.fit_predict(embedding_2d) # Changed X_umap to embedding_2d\n",
        "    n_clusters = len(set(labels_mcs)) - (1 if -1 in labels_mcs else 0)\n",
        "    print(f\"min_cluster_size={mcs} → {n_clusters} clusters\")\n",
        "#Objective: identify configurations that reveal small sub-clusters in the clearings.\n",
        "\n",
        "# Automatically detect low-density “pockets” via density estimation\n",
        "#KDE over UMAP\n",
        "#Use sklearn.neighbors.KernelDensity to estimate local density in the 2D UMAP space.\n",
        "\n",
        "#Find local minima\n",
        "#Apply a neighborhood filter (scipy’s maximum_filter/minimum_filter) over the density map to extract points that are density valleys (the bottoms of the clearings).\n",
        "\n",
        "#Cluster these valleys\n",
        "#With a small eps DBSCAN, group these minima into “sub-niches”.\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from scipy.ndimage import minimum_filter\n",
        "\n",
        "# 1) KDE\n",
        "kde = KernelDensity(bandwidth=0.5).fit(embedding_2d) # Changed X_umap to embedding_2d\n",
        "log_dens = kde.score_samples(embedding_2d) # Changed X_umap to embedding_2d\n",
        "\n",
        "# 2) Local minima\n",
        "mins = minimum_filter(log_dens, size=15)\n",
        "pockets = (log_dens == mins) & (log_dens < np.percentile(log_dens, 20))\n",
        "\n",
        "# 3) Extract indices and group with DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "valleys_idx = np.where(pockets)[0]\n",
        "db = DBSCAN(eps=0.3, min_samples=5).fit(embedding_2d[valleys_idx]) # Changed X_umap to embedding_2d\n",
        "sub_labels = db.labels_\n",
        "#Objective: automatically isolate points from the clearings and treat them as “Cluster D” of sub-niches.\n",
        "\n",
        "#Select these sub-niches for heuristic analysis\n",
        "#Once the indices of each sub-niche are identified (via HDBSCAN calibration or via KDE+DBSCAN)\n",
        "for subcluster in np.unique(sub_labels):\n",
        "     # ── FILTER: skip very small sub-niches ──\n",
        "    MIN_SIZE= 20\n",
        "    idxs = np.where(sub_labels == subcluster)[0]\n",
        "    if len(idxs) < MIN_SIZE:\n",
        "        print(f\"Ignoring sub-niche {subcluster}: only {len(idxs)} samples (<{MIN_SIZE})\")\n",
        "        continue\n",
        "    # Get the indices of the current subcluster within the 'valleys_idx'\n",
        "    current_subcluster_indices_in_valleys = np.where(sub_labels == subcluster)[0]\n",
        "    # Get the original indices of the samples belonging to the current subcluster\n",
        "    original_indices_of_subcluster = valleys_idx[current_subcluster_indices_in_valleys]\n",
        "\n",
        "    print(f\"\\n--- Sub-niche {subcluster}: {len(original_indices_of_subcluster)} images ---\")\n",
        "    print(f\"Average CLS: {cls_scores[original_indices_of_subcluster].mean():.2f}\")\n",
        "    print(f\"Average Uniqueness: {unicidade_scores[original_indices_of_subcluster].mean():.2f}\") # Changed uni_scores to unicidade_scores\n",
        "    print(f\"Average Originality: {originality_scores[original_indices_of_subcluster].mean():.2f}\") # Changed orig_scores to originality_scores\n",
        "\n",
        "    # Create a full-length boolean mask based on the original indices of the subcluster\n",
        "    full_length_mask = np.zeros(len(subset_attributes), dtype=bool)\n",
        "    full_length_mask[original_indices_of_subcluster] = True\n",
        "\n",
        "    # Use the full-length mask in compute_abi_scores\n",
        "    # Removed the target_label argument as the mask already specifies the target group\n",
        "    display(compute_abi_scores(subset_attributes, full_length_mask))\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Number of samples to plot per sub-niche\n",
        "N = 6\n",
        "base_path = \"/content/data/img_align_celeba/img_align_celeba\"\n",
        "\n",
        "for subcluster in np.unique(sub_labels):\n",
        "    idxs = np.where(sub_labels == subcluster)[0]\n",
        "    print(f\"\\n=== Sub-niche {subcluster}: {len(idxs)} images (Average CLS = {cls_scores[idxs].mean():.1f}) ===\")\n",
        "\n",
        "    # Choose N random samples\n",
        "    sample_idxs = np.random.choice(idxs, size=min(N, len(idxs)), replace=False)\n",
        "\n",
        "    # Check if there are any samples to plot before creating the figure\n",
        "    if len(sample_idxs) > 0:\n",
        "        # Plot\n",
        "        cols = 3\n",
        "        rows = int(np.ceil(len(sample_idxs)/cols))\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
        "        axes = axes.flatten()\n",
        "        for i, idx in enumerate(sample_idxs):\n",
        "            fname = file_names[idx]\n",
        "            img = Image.open(os.path.join(base_path, fname))\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].set_title(f\"CLS={cls_scores[idx]:.1f}\\nUn={unicidade_scores[idx]:.1f}, O={originality_scores[idx]:.1f}\")\n",
        "            axes[i].axis('off')\n",
        "        for j in range(i+1, len(axes)):\n",
        "            axes[j].axis('off')\n",
        "        plt.suptitle(f\"Examples — Sub-niche {subcluster}\", y=1.02, fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No samples to plot for subcluster {subcluster}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq2JwvxaGJI5"
      },
      "source": [
        "Identity Consistency Analysis using AUC (Area Under the ROC Curve) and Silhouette Score. It loads the identity mapping and selects identities with a sufficient number of images. It then generates positive pairs (images of the same identity) and negative pairs (images of different identities). It calculates a \"distance\" between pairs based on the heuristic scores and uses these scores and the generated labels (same or different identity) to compute the AUC, assessing how well the heuristics can distinguish between identities. It also calculates the Silhouette Score using the raw embeddings and identity labels to evaluate the clustering quality based on identity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRE6glNSW6yq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, silhouette_score\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import os # Import the os module\n",
        "\n",
        "# Load the identity mapping\n",
        "identity_file = \"/content/drive/MyDrive/Colab Notebooks/identity_CelebA.txt\"\n",
        "df_id = pd.read_csv(identity_file, sep=\" \", names=[\"image_id\", \"identity\"])\n",
        "\n",
        "# Create a dictionary mapping identity ID to a list of image indices in the subset\n",
        "# We need the mapping from image_id to original index in the subset\n",
        "image_id_to_subset_index = {os.path.basename(full_dataset.samples[i][0]): i for i in subset_indices}\n",
        "\n",
        "indices_per_id = {}\n",
        "# Iterate through the identity dataframe and populate the indices_per_id dictionary\n",
        "for index, row in df_id.iterrows():\n",
        "    image_id = row['image_id']\n",
        "    identity_id = row['identity']\n",
        "    if image_id in image_id_to_subset_index:\n",
        "        subset_index = image_id_to_subset_index[image_id]\n",
        "        if identity_id not in indices_per_id:\n",
        "            indices_per_id[identity_id] = []\n",
        "        indices_per_id[identity_id].append(subset_index)\n",
        "\n",
        "# --- AUC Identity Analysis ---\n",
        "print(\"--- Starting Identity Analysis with AUC ---\")\n",
        "# 1) Select K people with >= N photos (e.g., N=15)\n",
        "# Get identities with at least 15 images in the subset\n",
        "valid_ids = [pid for pid, indices in indices_per_id.items() if len(indices) >= 5]\n",
        "\n",
        "if not valid_ids:\n",
        "    print(\"No identities found with at least 15 images in the subset. Cannot perform AUC analysis.\")\n",
        "else:\n",
        "    # Use the first 5 valid identities\n",
        "    ids_to_use = valid_ids[:5]\n",
        "    print(f\"Using {len(ids_to_use)} identities for AUC analysis.\")\n",
        "\n",
        "\n",
        "    # Load the scores from the checkpoint DataFrame if not already loaded\n",
        "    try:\n",
        "        df_results = pd.read_parquet('checkpoint_final_results.parquet')\n",
        "        print(\"Checkpoint file loaded.\")\n",
        "        # Ensure scores are accessible by original index\n",
        "        unicidade_scores = df_results['Unicidade'].values[subset_indices]\n",
        "        originalidade_scores = df_results['Originalidade'].values[subset_indices]\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Checkpoint file 'checkpoint_final_results.parquet' not found.\")\n",
        "        print(\"Please run the checkpoint saving cell first.\")\n",
        "        unicidade_scores = None\n",
        "        originalidade_scores = None\n",
        "\n",
        "\n",
        "    if unicidade_scores is not None and originalidade_scores is not None:\n",
        "        # 2) Generate positive and negative pairs\n",
        "        pairs = []\n",
        "        labels = []\n",
        "\n",
        "        # Positive pairs (same identity)\n",
        "        for pid in ids_to_use:\n",
        "            imgs = indices_per_id[pid]\n",
        "            for i, j in itertools.combinations(imgs, 2):\n",
        "                pairs.append((i, j))\n",
        "                labels.append(1)\n",
        "\n",
        "        # Negative pairs (different identities) - sample a reasonable number\n",
        "        num_negative_pairs_per_id_pair = 50 # Limit negative pairs to avoid imbalance\n",
        "        for pid1, pid2 in itertools.combinations(ids_to_use, 2):\n",
        "            # Sample images from each identity to create negative pairs\n",
        "            imgs1 = np.random.choice(indices_per_id[pid1], size=min(len(indices_per_id[pid1]), 10), replace=False) # Sample up to 10 images\n",
        "            imgs2 = np.random.choice(indices_per_id[pid2], size=min(len(indices_per_id[pid2]), 10), replace=False) # Sample up to 10 images\n",
        "\n",
        "            neg_pairs_count = 0\n",
        "            for i in imgs1:\n",
        "                 for j in imgs2:\n",
        "                      pairs.append((i, j))\n",
        "                      labels.append(0)\n",
        "                      neg_pairs_count += 1\n",
        "                      if neg_pairs_count >= num_negative_pairs_per_id_pair:\n",
        "                          break # Limit negative pairs per id pair\n",
        "                 if neg_pairs_count >= num_negative_pairs_per_id_pair:\n",
        "                     break\n",
        "\n",
        "\n",
        "        print(f\"Generated {len(pairs)} pairs ({labels.count(1)} positive, {labels.count(0)} negative).\")\n",
        "\n",
        "        if len(pairs) > 0:\n",
        "            # 3) Calculate the \"distance\" based on heuristics\n",
        "            scores = []\n",
        "            for i, j in pairs:\n",
        "                d_uni = abs(unicidade_scores[i] - unicidade_scores[j])\n",
        "                d_ori = abs(originalidade_scores[i] - originality_scores[j])\n",
        "                scores.append((d_uni + d_ori) / 2)  # or another combination\n",
        "\n",
        "            # 4) ROC AUC\n",
        "            auc = roc_auc_score(labels, scores)\n",
        "            print(f\"Identity AUC (Based on Heuristics): {auc:.3f}\")\n",
        "        else:\n",
        "            print(\"No pairs generated. Cannot compute AUC.\")\n",
        "    else:\n",
        "        print(\"Scores not available. Cannot compute AUC.\")\n",
        "\n",
        "\n",
        "# --- Silhouette Score Analysis ---\n",
        "print(\"\\n--- Starting Identity Consistency Analysis with Silhouette Score ---\")\n",
        "\n",
        "# 1. Filter embeddings and identity labels for identities with enough images\n",
        "# We need the raw embeddings (real_embeddings_numpy) and corresponding identity labels\n",
        "\n",
        "# Get indices of all samples belonging to the identities used for AUC analysis\n",
        "indices_for_silhouette = []\n",
        "identity_labels_for_silhouette = []\n",
        "\n",
        "# Collect indices and identity labels for all images belonging to the identities used for AUC\n",
        "for pid in ids_to_use:\n",
        "    indices = indices_per_id[pid]\n",
        "    indices_for_silhouette.extend(indices)\n",
        "    identity_labels_for_silhouette.extend([pid] * len(indices))\n",
        "\n",
        "# Ensure we have enough samples (at least 2) and more than one unique label for Silhouette Score\n",
        "if len(indices_for_silhouette) < 2 or len(set(identity_labels_for_silhouette)) < 2:\n",
        "    print(\"Not enough samples or unique identities to compute Silhouette Score.\")\n",
        "else:\n",
        "    # Get the corresponding raw embeddings for these indices\n",
        "    embeddings_for_silhouette = real_embeddings_numpy[indices_for_silhouette]\n",
        "\n",
        "    # Calculate the Silhouette Score\n",
        "    # Use the raw embeddings and the identity labels as clusters\n",
        "    silhouette_avg = silhouette_score(embeddings_for_silhouette, identity_labels_for_silhouette)\n",
        "\n",
        "    print(f\"Silhouette Score (Embeddings vs. Identity Labels): {silhouette_avg:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-EnZ0FZGhHo"
      },
      "source": [
        "This cell performs non-parametric statistical tests (Mann-Whitney U and Kolmogorov-Smirnov) to compare the distributions of Originality scores between two groups (presumably Celebrity A and Celebrity B, based on the variable names `scores_grupo_A` and `scores_grupo_B`, although there was a `NameError` in a previous attempt). It also includes a bootstrap analysis to estimate the confidence interval for the difference in variances of Originality between these groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmJQaQ_Yggbi"
      },
      "outputs": [],
      "source": [
        "#Non-Parametric Distribution Tests\n",
        "from scipy.stats import mannwhitneyu, ks_2samp\n",
        "\n",
        "# Having the numpy arrays of Originality\n",
        "# Access the Originalidade scores from the dictionaries\n",
        "orig_A = scores_group_A['Originality']\n",
        "orig_B = scores_group_B['Originality']\n",
        "\n",
        "\n",
        "u_stat, p_mw = mannwhitneyu(orig_A, orig_B, alternative='two-sided')\n",
        "ks_stat, p_ks = ks_2samp(orig_A, orig_B)\n",
        "\n",
        "print(f\"Mann-Whitney U: stat={u_stat:.1f}, p={p_mw:.3f}\")\n",
        "print(f\"Kolmogorov-Smirnov: stat={ks_stat:.3f}, p={p_ks:.3f}\")\n",
        "\n",
        "#Mann-Whitney U tests if one distribution tends to result in larger values than the other, without assuming normality.\n",
        "\n",
        "#KS test compares the entire shape of the distribution.\n",
        "\n",
        "#Bootstrap to Estimate Confidence Intervals\n",
        "import numpy as np\n",
        "\n",
        "def bootstrap_statistic(data1, data2, stat_fn, n_boot=1000):\n",
        "    \"\"\"\n",
        "    Returns the bootstrap distribution of stat_fn(data1) - stat_fn(data2).\n",
        "    \"\"\"\n",
        "    diffs = []\n",
        "    combined = np.concatenate([data1, data2])\n",
        "    n1, n2 = len(data1), len(data2)\n",
        "    for _ in range(n_boot):\n",
        "        # Resampling with replacement\n",
        "        b1 = np.random.choice(data1, size=n1, replace=True)\n",
        "        b2 = np.random.choice(data2, size=n2, replace=True)\n",
        "        diffs.append(stat_fn(b1) - stat_fn(b2))\n",
        "    return np.array(diffs)\n",
        "\n",
        "# Example: difference of variances of Originality\n",
        "boot_diffs = bootstrap_statistic(orig_A, orig_B, np.var, n_boot=2000)\n",
        "ci_lower, ci_upper = np.percentile(boot_diffs, [2.5, 97.5])\n",
        "print(f\"95% CI of variance difference: [{ci_lower:.2f}, {ci_upper:.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jkb7FY_HF1R"
      },
      "source": [
        "This cell reloads the analysis results from the Parquet checkpoint file and ensures the 'CLS' column exists. It then calculates the percentile thresholds for Uniqueness and Originality scores (e.g., the 25th percentile). Finally, it creates a mask to identify \"generic\" samples as those falling below both the Uniqueness and Originality thresholds and creates a new DataFrame containing only these generic samples, along with their count and percentage of the total dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gseJbTOSujsM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the results from our secure checkpoint\n",
        "try:\n",
        "    df_results = pd.read_parquet('checkpoint_final_results.parquet')\n",
        "    print(\"Checkpoint file loaded successfully!\")\n",
        "    print(f\"Total samples loaded: {len(df_results)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: The file 'checkpoint_final_results.parquet' was not found.\")\n",
        "    print(\"Please ensure the saving cell was executed successfully.\")\n",
        "    # Stop execution if the file does not exist\n",
        "    # df_results = None\n",
        "\n",
        "# Ensure the dataframe was loaded before proceeding\n",
        "if 'df_results' in locals() and df_results is not None:\n",
        "\n",
        "    # QUESTION: Does your dataframe already have the 'CLS' column?\n",
        "    # If not, we need to calculate it first.\n",
        "    # Assuming yes, and that the score columns are named 'Unicidade' and 'Originalidade'\n",
        "    if 'CLS' not in df_results.columns:\n",
        "        # If the CLS column does not exist, let's create it.\n",
        "        # Adapt the names 'Unicidade' and 'Originalidade' if they are different.\n",
        "        df_results['CLS'] = (df_results['Unicidade'] + df_results['Originalidade']) / 2 # Changed from 'Uniqueness' and 'Originality' to 'Unicidade' and 'Originalidade'\n",
        "        print(\"'CLS' column calculated and added.\")\n",
        "\n",
        "    # Filter out noise (cluster -1) to not include in the mean calculation\n",
        "    df_clusters_only = df_results[df_results['cluster_label'] != -1].copy()\n",
        "\n",
        "    # Calculate the average CLS per cluster\n",
        "    avg_cls_per_cluster = df_clusters_only.groupby('cluster_label')['CLS'].mean().sort_values(ascending=False)\n",
        "\n",
        "    # Identify the cluster with the highest average CLS\n",
        "    creative_cluster_id = avg_cls_per_cluster.idxmax()\n",
        "\n",
        "    print(\"\\n--- Creative Score (CLS) Analysis by Cluster ---\")\n",
        "    print(avg_cls_per_cluster)\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(f\"\\n🏆 The Creative Cluster (Cluster C) is: CLUSTER {creative_cluster_id}\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "\n",
        "    # Step 1: Define the percentile threshold (25% is a good start)\n",
        "    percentile_threshold = 0.25\n",
        "\n",
        "    # Step 2: Calculate the value of the scores at this percentile\n",
        "    unicidade_threshold = df_results['Unicidade'].quantile(percentile_threshold)\n",
        "    originalidade_threshold = df_results['Originalidade'].quantile(percentile_threshold)\n",
        "\n",
        "    print(f\"Uniqueness Threshold (Percentile {percentile_threshold*100}%): {unicidade_threshold:.4f}\")\n",
        "    print(f\"Originality Threshold (Percentile {percentile_threshold*100}%): {originalidade_threshold:.4f}\")\n",
        "\n",
        "    # Step 3: Create a mask to select the \"generic\" images\n",
        "    mask_generic = (df_results['Unicidade'] < unicidade_threshold) & \\\n",
        "                    (df_results['Originalidade'] < originalidade_threshold)\n",
        "\n",
        "    # Step 4: Create the new DataFrame with only the generic samples\n",
        "    df_generic = df_results[mask_generic].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    print(f\"\\nTotal samples in the complete dataset: {len(df_results)}\")\n",
        "    print(f\"Number of samples in the 'generic' subset: {len(df_generic)}\")\n",
        "    print(f\"Percentage of the dataset selected: {(len(df_generic) / len(df_results) * 100):.2f}%\")\n",
        "\n",
        "    # Now, 'df_generic' is ready for identity analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "444b5815"
      },
      "outputs": [],
      "source": [
        "#=============================\n",
        "# CHECKPOINT CREATION CELL (v2.1 - CORRECTED)\n",
        "# Execute this cell to save all final analysis results.\n",
        "# ==========================================================\n",
        "print(\"Starting final checkpoint creation...\")\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# Load necessary variables if they don't exist (assuming the user might restart runtime)\n",
        "try:\n",
        "    _ = full_dataset\n",
        "    _ = subset_indices\n",
        "    _ = clusters_umap # Check if clusters_umap exists from UMAP+HDBSCAN\n",
        "    _ = real_embeddings_numpy # Check if real_embeddings_numpy exists\n",
        "    _ = df_attributes # Check if df_attributes exists\n",
        "    print(\"Necessary variables (full_dataset, subset_indices, clusters_umap, real_embeddings_numpy, df_attributes) found.\")\n",
        "except NameError:\n",
        "    print(\"One or more necessary variables not found. Attempting to load/recreate minimal data...\")\n",
        "    # --- Recreate minimal necessary variables ---\n",
        "    base_dataset_path = '/content/celeba_images/img_align_celeba/'\n",
        "    basic_transforms = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
        "    full_dataset = datasets.ImageFolder(root=base_dataset_path, transform=basic_transforms)\n",
        "    subset_indices = list(range(10000))\n",
        "    print(\"full_dataset and subset_indices recreated.\")\n",
        "\n",
        "    # Load attributes file\n",
        "    attributes_path = \"/content/celeba_images/list_attr_celeba.csv\"\n",
        "    try:\n",
        "        df_attributes = pd.read_csv(attributes_path).set_index('image_id')\n",
        "        print(\"Attributes file loaded.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Attributes file not found at '{attributes_path}'. Cannot save full checkpoint.\")\n",
        "        df_attributes = pd.DataFrame() # Create empty df to avoid errors later\n",
        "\n",
        "    # Note: real_embeddings_numpy and clusters_umap cannot be reliably recreated here.\n",
        "    # The user must run the relevant cells to generate them if they are missing.\n",
        "    print(\"real_embeddings_numpy and clusters_umap are NOT recreated. Please run the relevant cells.\")\n",
        "    real_embeddings_numpy = None\n",
        "    clusters_umap = None # Ensure it's None if not found\n",
        "\n",
        "\n",
        "# --- Recalculate Uniqueness and Originality from Embeddings ---\n",
        "# This ensures the scores are based on the actual embeddings used, even if the heuristic objects are gone\n",
        "if real_embeddings_numpy is not None:\n",
        "    print(\"\\nRecalculating Uniqueness and Originality scores...\")\n",
        "\n",
        "    # Uniqueness (based on average distance to other points)\n",
        "    # Calculate pairwise distances (Euclidean is common for this)\n",
        "    distances = pdist(real_embeddings_numpy, metric='euclidean')\n",
        "    square_distances = squareform(distances)\n",
        "\n",
        "    # Calculate average distance for each point to all others\n",
        "    # Add a small epsilon to avoid division by zero if somehow a point has no distance to others (shouldn't happen)\n",
        "    avg_distances = (np.sum(square_distances, axis=1) / (real_embeddings_numpy.shape[0] - 1)) + 1e-8 # Exclude distance to self\n",
        "\n",
        "    # A higher average distance means higher uniqueness (more isolated)\n",
        "    uniqueness_scores = avg_distances\n",
        "\n",
        "    # Originality (based on local density - inverse of distance to k-nearest neighbors)\n",
        "    # Re-using the logic from LatentSpaceAnalyzer's calculate_density_map, but inverse\n",
        "    k = 10 # Number of neighbors to consider for density\n",
        "    if k >= len(real_embeddings_numpy):\n",
        "         print(f\"Warning: k ({k}) is too large for Originality calculation. Adjusting k.\")\n",
        "         k = len(real_embeddings_numpy) - 1\n",
        "         if k < 1:\n",
        "              print(\"Error: Not enough samples for Originality calculation.\")\n",
        "              originality_scores = np.zeros(len(real_embeddings_numpy))\n",
        "         else:\n",
        "              neighbors_model = NearestNeighbors(n_neighbors=k + 1, algorithm='kd_tree', leaf_size=30, n_jobs=-1)\n",
        "              neighbors_model.fit(real_embeddings_numpy)\n",
        "              distances_knn, _ = neighbors_model.kneighbors(real_embeddings_numpy)\n",
        "              # Average distance to k neighbors (excluding self)\n",
        "              avg_knn_distance = np.mean(distances_knn[:, 1:], axis=1) + 1e-8\n",
        "              # Originality is higher in low-density areas, so it's related to higher average KNN distance\n",
        "              originality_scores = avg_knn_distance # Use average distance as originality score (higher distance = higher originality)\n",
        "\n",
        "    else:\n",
        "        neighbors_model = NearestNeighbors(n_neighbors=k + 1, algorithm='kd_tree', leaf_size=30, n_jobs=-1)\n",
        "        neighbors_model.fit(real_embeddings_numpy)\n",
        "        distances_knn, _ = neighbors_model.kneighbors(real_embeddings_numpy)\n",
        "        avg_knn_distance = np.mean(distances_knn[:, 1:], axis=1) + 1e-8\n",
        "        originality_scores = avg_knn_distance\n",
        "\n",
        "\n",
        "    print(\"Uniqueness and Originality scores recalculated.\")\n",
        "else:\n",
        "    print(\"\\nCannot recalculate Uniqueness and Originality: real_embeddings_numpy is not available.\")\n",
        "    uniqueness_scores = None\n",
        "    originality_scores = None\n",
        "\n",
        "\n",
        "# --- Prepare the final DataFrame ---\n",
        "# We need the attributes for the subset\n",
        "subset_attributes = pd.DataFrame()\n",
        "if not df_attributes.empty:\n",
        "    file_names = [os.path.basename(full_dataset.samples[i][0]) for i in subset_indices]\n",
        "    # Ensure file names from subset_indices exist in the attributes index\n",
        "    existing_files = [name for name in file_names if name in df_attributes.index]\n",
        "    if len(existing_files) != len(file_names):\n",
        "        print(f\"Warning: {len(file_names) - len(existing_files)} subset file names not found in attributes file. Filtering.\")\n",
        "        file_names = existing_files # Filter file_names to only include existing ones\n",
        "\n",
        "    if file_names:\n",
        "         subset_attributes = df_attributes.loc[file_names].copy()\n",
        "         subset_attributes = (subset_attributes + 1) / 2 # Convert -1/1 to 0/1\n",
        "         print(f\"Subset attributes prepared: {subset_attributes.shape}\")\n",
        "    else:\n",
        "         print(\"No matching file names found between subset_indices and attributes file. Cannot include attributes.\")\n",
        "else:\n",
        "    print(\"df_attributes is empty. Cannot include attributes in checkpoint.\")\n",
        "\n",
        "\n",
        "# Create the main DataFrame to save\n",
        "if not subset_attributes.empty:\n",
        "    df_to_save = subset_attributes.copy()\n",
        "\n",
        "    # Add cluster labels if available\n",
        "    if clusters_umap is not None and len(clusters_umap) == len(subset_attributes):\n",
        "        df_to_save['cluster_label'] = clusters_umap\n",
        "        print(\"'cluster_label' added.\")\n",
        "    else:\n",
        "        df_to_save['cluster_label'] = -2 # Use -2 to indicate missing/unavailable cluster label\n",
        "        print(\"'cluster_label' not available or size mismatch. Added with -2.\")\n",
        "\n",
        "    # Add heuristic scores if available and size matches\n",
        "    if uniqueness_scores is not None and len(uniqueness_scores) == len(subset_attributes):\n",
        "         df_to_save['Unicidade'] = uniqueness_scores[subset_indices] # Ensure scores are for the subset\n",
        "         print(\"'Unicidade' added.\")\n",
        "    else:\n",
        "         df_to_save['Unicidade'] = np.nan # Use NaN if scores are missing\n",
        "         print(\"'Unicidade' not available or size mismatch. Added with NaN.\")\n",
        "\n",
        "    if originality_scores is not None and len(originality_scores) == len(subset_attributes):\n",
        "        df_to_save['Originalidade'] = originality_scores[subset_indices] # Ensure scores are for the subset\n",
        "        print(\"'Originalidade' added.\")\n",
        "    else:\n",
        "        df_to_save['Originalidade'] = np.nan # Use NaN if scores are missing\n",
        "        print(\"'Originalidade' not available or size mismatch. Added with NaN.\")\n",
        "\n",
        "\n",
        "    # Calculate CLS if Unicidade and Originalidade are available\n",
        "    if 'Unicidade' in df_to_save.columns and 'Originalidade' in df_to_save.columns and not df_to_save[['Unicidade', 'Originalidade']].isnull().all().all():\n",
        "        # Only calculate CLS if both scores are present for at least one row\n",
        "        df_to_save['CLS'] = (df_to_save['Unicidade'] + df_to_save['Originalidade']) / 2\n",
        "        print(\"'CLS' calculated and added.\")\n",
        "    else:\n",
        "        df_to_save['CLS'] = np.nan # Use NaN if CLS cannot be calculated\n",
        "        print(\"'CLS' not calculated: Unicidade or Originalidade scores missing.\")\n",
        "\n",
        "\n",
        "    # --- Save the final DataFrame ---\n",
        "    final_file_path = 'checkpoint_TUDO_INCLUIDO.parquet'\n",
        "    try:\n",
        "        df_to_save.to_parquet(final_file_path)\n",
        "        print(\"\\n--- FINAL CHECKPOINT SUCCESS! ---\")\n",
        "        print(f\"Comprehensive checkpoint saved to: '{final_file_path}'\")\n",
        "        print(f\"DataFrame shape: {df_to_save.shape}\")\n",
        "        print(\"==========================================================\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR SAVING CHECKPOINT: {e}\")\n",
        "        print(\"Could not save the final checkpoint file.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nCannot create the final DataFrame to save. Subset attributes not available.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhvSPkaa7ALmIyj0t8I2xE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}